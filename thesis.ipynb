{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thesis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fk5tZXq9wVWa",
        "0UzeYFmhVYyR",
        "ObfLNgfoRPIR",
        "Kdve_He2A_qK",
        "lNfIfKr8jA-3",
        "4piQ486FPXjy",
        "B4ugjeWMPaed",
        "UZ8cYvBPfNgF",
        "fbaP1niJUcA0",
        "yMfPab2HXIr4",
        "XVWquCoX-e-b",
        "2u9KTwaM-j_9",
        "g09GeoJF6p5C",
        "7VPM1-T2oJdf",
        "_yMXKMiKmQdG",
        "tzEeAJLA0KPN",
        "O1W30E_WYWew",
        "3bByP0YCUgw7",
        "t9yj5rYkUj7Q",
        "rH-tkyCOUrhM",
        "Hs_7XceCU_vz",
        "i0I9kkGWmLIV",
        "uRkIusfSJNCj",
        "8Uv3x3yZJZ0l",
        "Vy-JR2DNtpQr",
        "mnAoWyAYvAT1",
        "NlbbnUdBQ_rQ",
        "q9h-2T3F_yoX",
        "gJfvVW3xxI9x",
        "kQNgqC2eT25U",
        "4LtWEZWmfAbb",
        "ZcC_AYiqMxz-",
        "JP9c2rO3U7b4",
        "CNT1mD6NPTgJ",
        "fZl5puPBN3us",
        "tyds0__agbbN",
        "Dskyp2RsuJyo",
        "vOe1-N82w011",
        "2Plw3V3-w4vg",
        "w0ogtx-g1N7r",
        "LG1N77Yxw4Vu",
        "BovXnJtNBScH",
        "PpLVGFMM0-rB",
        "BnBwnr2uFkEj",
        "CGBj97ZDWjAJ",
        "O7mj3exEWoeM",
        "AtlEF4j5UZ0U",
        "JLH2Gopjb8UT",
        "EJTKHGULix-I",
        "6tNVEP7ki1Ne",
        "_WggyjQWcaiW",
        "hEhdlaoRZU3d",
        "WBE2ku0WkNLO",
        "Kjso2pdAZ92Z",
        "2n8-TqXHZ5ty",
        "x4J8xtGO8sTr",
        "ecnPuER18vl9",
        "5L7LvlxM80sy",
        "fN-59oLQ84hz",
        "bdhm0q6S6HCA",
        "z_bqW9H-kCCa"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk5tZXq9wVWa"
      },
      "source": [
        "# **Installations, Imports, Mounting: run me!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxC6HV_aUPNa"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# INSTALLATIONS\n",
        "# ------------------------------------------------------------------------------\n",
        "! pip install tweepy --upgrade # for version with 'tweet_mode' param\n",
        "! pip install emoji\n",
        "! pip install jsonlines\n",
        "! pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbEpuql1wa1V"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# IMPORTS\n",
        "# ------------------------------------------------------------------------------\n",
        "import ast\n",
        "from csv import reader\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import io\n",
        "import json\n",
        "import jsonlines\n",
        "import math\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "import re\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import string\n",
        "import subprocess\n",
        "from time import sleep\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "import tweepy\n",
        "import vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE-gjGjUUQqq"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# MOUNT TO GOOGLE DRIVE (use princeton account)\n",
        "# ------------------------------------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UzeYFmhVYyR"
      },
      "source": [
        "# **State Setup: run me!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul9y4DuAwNpc"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# SAVE STATE ABBREVIATIONS FOR REFERENCE\n",
        "# ------------------------------------------------------------------------------\n",
        "# lower & uppercase, ordered alphabetically (index is fips code)\n",
        "states_lower = ['al', 'ak', 'az', 'ar', 'ca', 'co', 'ct', 'de', 'fl', 'ga', \n",
        "                'hi', 'id', 'il', 'in', 'ia', 'ks', 'ky', 'la', 'me', 'md', \n",
        "                'ma', 'mi', 'mn', 'ms', 'mo', 'mt', 'ne', 'nv', 'nh', 'nj', \n",
        "                'nm', 'ny', 'nc', 'nd', 'oh', 'ok', 'or', 'pa', 'ri', 'sc', \n",
        "                'sd', 'tn', 'tx', 'ut', 'vt', 'va', 'wa', 'wv', 'wi', 'wy']\n",
        "states_upper = [state.upper() for state in states_lower] # uppercase\n",
        "\n",
        "# territories (not currently using but keeping my options open!)\n",
        "terrs_lower = ['as', 'dc', 'fm', 'gu', 'mh', 'mp', 'pw', 'pr', 'vi']\n",
        "terrs_upper = [terr.upper() for terr in terrs_lower] # uppercase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5xd0T6IRAZ0"
      },
      "source": [
        "# dict of state name to abbreviation (all lowercase)\n",
        "states_abbrev = {\n",
        "    'alabama': 'al',\n",
        "    'alaska': 'ak',\n",
        "    'arizona': 'az',\n",
        "    'arkansas': 'ar',\n",
        "    'california': 'ca',\n",
        "    'colorado': 'co',\n",
        "    'connecticut': 'ct',\n",
        "    'delaware': 'de',\n",
        "    'florida': 'fl',\n",
        "    'georgia': 'ga',\n",
        "    'hawaii': 'hi',\n",
        "    'idaho': 'id',\n",
        "    'illinois': 'il',\n",
        "    'indiana': 'in',\n",
        "    'iowa': 'ia',\n",
        "    'kansas': 'ks',\n",
        "    'kentucky': 'ky',\n",
        "    'louisiana': 'la',\n",
        "    'maine': 'me',\n",
        "    'maryland': 'md',\n",
        "    'massachusetts': 'ma',\n",
        "    'michigan': 'mi',\n",
        "    'minnesota': 'mn',\n",
        "    'mississippi': 'ms',\n",
        "    'missouri': 'mo',\n",
        "    'montana': 'mt',\n",
        "    'nebraska': 'ne',\n",
        "    'nevada': 'nv',\n",
        "    'new hampshire': 'nh',\n",
        "    'new jersey': 'nj',\n",
        "    'new mexico': 'nm',\n",
        "    'new york': 'ny',\n",
        "    'north carolina': 'nc',\n",
        "    'north dakota': 'nd',\n",
        "    'ohio': 'oh',\n",
        "    'oklahoma': 'ok',\n",
        "    'oregon': 'or',\n",
        "    'pennsylvania': 'pa',\n",
        "    'rhode island': 'ri',\n",
        "    'south carolina': 'sc',\n",
        "    'south dakota': 'sd',\n",
        "    'tennessee': 'tn',\n",
        "    'texas': 'tx',\n",
        "    'utah': 'ut',\n",
        "    'vermont': 'vt',\n",
        "    'virginia': 'va',\n",
        "    'washington': 'wa',\n",
        "    'west virginia': 'wv',\n",
        "    'wisconsin': 'wi',\n",
        "    'wyoming': 'wy'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObfLNgfoRPIR"
      },
      "source": [
        "# **COVID Tracking Project: Data Collection**\n",
        "This section accumulates COVID-19 data for all 50 states including official Twitter handles for each state and historical case data.\n",
        "\n",
        "API Information: https://covidtracking.com/data/api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdve_He2A_qK"
      },
      "source": [
        "## Official COVID-19 Twitter handles for each state\n",
        "(e.g. state health departments). This can be used later to query the Twitter API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRxJafzdxPiB"
      },
      "source": [
        "query_url = ('https://api.covidtracking.com/v1/states/info.json')\n",
        "response = requests.get(query_url)\n",
        "\n",
        "# create dict {state_name: handle}\n",
        "# name: upper case abbreviation, value: None if no handle exists\n",
        "# includes the 50 states + 6 territories\n",
        "payload = response.json()\n",
        "df = pd.DataFrame(payload)\n",
        "state_twitters = dict(zip(df.state, df.twitter))\n",
        "\n",
        "# print(state_twitters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkfdMiMzejh4"
      },
      "source": [
        "state_twitters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNfIfKr8jA-3"
      },
      "source": [
        "## Historical COVID-19 Case Data for States\n",
        "Note: no need to run this unless want data past 2020. Code to get saved data from GDrive below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4piQ486FPXjy"
      },
      "source": [
        "###Individual State: Example and Visualization\n",
        "Used for getting a sense for the type, form, and trends of data for each state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0KxQHT_BzQp"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# HISTORICAL STATE DATA EXAMPLE (start date varies, end date yesterday)\n",
        "# ------------------------------------------------------------------------------\n",
        "query_url = ('https://api.covidtracking.com/v2/states/ct/daily.json')\n",
        "response = requests.get(query_url)\n",
        "payload = response.json()\n",
        "\n",
        "# df = pd.DataFrame(payload) # create dataframe of info\n",
        "# pd.set_option('display.max_rows', None) # Change to None to see all rows\n",
        "# df # display dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6OE0BXRbkGj"
      },
      "source": [
        "payload['data'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFMdBrrebN3-"
      },
      "source": [
        "for entry in payload['data']:\n",
        "  print(entry['date'], entry['meta']['data_quality_grade'], entry['cases']['total']['value'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fskp3snwNl4w"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# DATA VISUALIZATION EXAMPLE\n",
        "# ------------------------------------------------------------------------------\n",
        "data = df['positiveIncrease'] # can change what category to visualize here\n",
        "data.time = pd.to_datetime(df['date'], format='%Y%m%d')\n",
        "graph = data.plot(title=\"NJ Daily Increase in Positive COVID-19 Cases\")\n",
        "graph.set_xlabel('days ago')\n",
        "graph.set_ylabel('positive case count')\n",
        "plt.gca().invert_xaxis()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4ugjeWMPaed"
      },
      "source": [
        "### Query Historical Data and save to CSV files by state in GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1_7Yct1JWES"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# QUERY HISTORICAL DATA, SAVE INTO DICT\n",
        "# ------------------------------------------------------------------------------\n",
        "# Key: lowercase state abbreviation\n",
        "# Value: dataframe with values, going backwards in date\n",
        "state_historical = {}\n",
        "for state in states_lower:\n",
        "  url = ('https://api.covidtracking.com/v1/states/' + state + '/daily.json')\n",
        "  response = requests.get(url)\n",
        "\n",
        "  payload = response.json()\n",
        "  df = pd.DataFrame(payload)\n",
        "  # Note: 'negativeIncrease' category is deprecated\n",
        "  state_historical[state] = df[['date', 'positive', 'positiveIncrease', 'negative', 'hospitalizedIncrease', 'deathIncrease', 'dataQualityGrade']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYKn0i4VC9Fb"
      },
      "source": [
        "# convert dict to master df\n",
        "state_historical_df = pd.DataFrame(list(state_historical.values()), index=state_historical.keys(), columns = ['info']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cBTFP7wPvra"
      },
      "source": [
        "# example indexing into data for reference\n",
        "state_historical_df.loc['nj']['info']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYAcNDzgUvLU"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# SAVE HISTORICAL DATA DF TO CSV FILES IN GDRIVE\n",
        "# ------------------------------------------------------------------------------\n",
        "for state in states_lower:\n",
        "  date = '2021-03-12' # put current date as string in format'YYYY-MM-DD'. Create folder with this name in GDrive before running.\n",
        "  state_historical_df.loc[state]['info'].to_csv('{}_{}_historical.csv'.format(date, state))\n",
        "  !cp $date\"_\"$state\"_historical.csv\" \"drive/My Drive/Thesis/\"$date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ8cYvBPfNgF"
      },
      "source": [
        "##Query Data Quality Grades from V2 of API and Save to CSVs  \n",
        "Data collected separately because V1 of the API stopped supporting this field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bckMGDHlfd1E"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# QUERY DATA QUALITY GRADES, SAVE INTO DICT\n",
        "# ------------------------------------------------------------------------------\n",
        "# Key: lowercase state abbreviation\n",
        "# Value: dataframe with values, going backwards in date\n",
        "state_grades = {}\n",
        "for state in states_lower:\n",
        "  url = ('https://api.covidtracking.com/v2/states/' + state + '/daily.json')\n",
        "  response = requests.get(url)\n",
        "  payload = response.json()\n",
        "\n",
        "  grades = []\n",
        "  for entry in payload['data']:\n",
        "    grades.append([entry['date'], entry['meta']['data_quality_grade']])\n",
        "\n",
        "  state_df = pd.DataFrame(grades)\n",
        "  state_df.columns=['date', 'dataQualityGrade']\n",
        "  state_grades[state] = state_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMuAQ6FjhQWB"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# SAVE DATA QUALITY GRADES DF TO CSV FILES IN GDRIVE\n",
        "# ------------------------------------------------------------------------------\n",
        "for state in states_lower:\n",
        "  state_grades[state].to_csv('{}_dataQualityGrade.csv'.format(state))\n",
        "  !cp $state\"_dataQualityGrade.csv\" \"drive/My Drive/Thesis/dataQualityGrades\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMfPab2HXIr4"
      },
      "source": [
        "# **CoronaVis: Twitter Data Collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVWquCoX-e-b"
      },
      "source": [
        "## Authentication, set up\n",
        "Run before any requests to API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skif475ViTBk"
      },
      "source": [
        "# Twitter Developer Authentication Info\n",
        "# Fill in your info here\n",
        "API_key = ''\n",
        "API_secret_key = ''\n",
        "bearer_token = ''\n",
        "access_token = ''\n",
        "access_token_secret = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYt3ZG8_itOq"
      },
      "source": [
        "# Tweepy setup\n",
        "auth = tweepy.OAuthHandler(API_key, API_secret_key)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u9KTwaM-j_9"
      },
      "source": [
        "## 1. CoronaVis: Hydration\n",
        "NOTE: 05-05-2020 done separately\n",
        "\n",
        "Tweet IDs from https://github.com/mykabir/COVID19. Paper compiled COVID-19 related tweet IDs by date "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g09GeoJF6p5C"
      },
      "source": [
        "### Example Tweet Query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCBJJXHYBQtq"
      },
      "source": [
        "tweet = api.statuses_lookup([1258116423924289536], tweet_mode=\"extended\")\n",
        "tweet[0]._json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VPM1-T2oJdf"
      },
      "source": [
        "### Check Twitter API Limits\n",
        "Make sure to run this before calling functions that check API limits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQBGu8pzb9N_"
      },
      "source": [
        "# use to check current limits on api calls (900/session, 15min sessions)\n",
        "def checkLimit():\n",
        "  return api.rate_limit_status()['resources']['statuses']['/statuses/lookup']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l67mSQ3G8rvh"
      },
      "source": [
        "checkLimit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yMXKMiKmQdG"
      },
      "source": [
        "### Obtain and Hydrate Tweets\n",
        "DONE! Ran for CoronaVis IDs on dates 03-05-2020 to 05-05-2020. Ran twarc locally on dates 05-06-2020 to 08-07-2020 (data folder) and 08-07-2020 to 12-31-2020 (data2 folder). To do this, just `cd` into the applicable folder and run `bash twarc_hydrate.sh`. Then upload to google drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyDA7WDaLA_y"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# DOWNLOAD DEHYDRATED TWEET IDS (GDrive folder, originally from the Git repo)\n",
        "# coronavis_ids dict: {yyyy-mm-dd: tweet_ids}\n",
        "# Run this code cell before count analyses in the next section\n",
        "# ------------------------------------------------------------------------------\n",
        "coronavis_files = glob.glob(\"drive/My Drive/Thesis/CoronaVis/*.csv\")\n",
        "coronavis_ids = {}\n",
        "for f in coronavis_files:\n",
        "  date = f[-14:-4]\n",
        "  coronavis_ids[date] = pd.read_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAV7qOf16pav"
      },
      "source": [
        "# initialize request count variable before calling hydrating function\n",
        "request_count = 900 - checkLimit()['remaining']\n",
        "request_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl2Z3PK8Ppqi"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# TWEET HYDRATING FUNCTION. Called by next code cell.\n",
        "# ------------------------------------------------------------------------------\n",
        "# given csv file of tweet IDs, returns df with index=id and the following cols: \n",
        "# tweet full text, user_loc (user defined location on their profile), place\n",
        "# (tweet location), coords (tweet coords in geoJSON longitude, latitude)\n",
        "def hydrateTweetCSV(file, curr_request_count):\n",
        "  tweet_df = pd.DataFrame()\n",
        "  request_count = curr_request_count\n",
        "\n",
        "  for i in range(int(len(file['tweet_id']) / 100)):\n",
        "    # print to get a sense for timing\n",
        "    if request_count % 100 == 0 and request_count != 0:\n",
        "      print(datetime.now().time().strftime('%H:%M:%S'), 'finished 100 requests')\n",
        "    # 15min session limit of 900 requests\n",
        "    if request_count % 900 == 0 and request_count != 0:\n",
        "      print(datetime.now().time().strftime('%H:%M:%S'), 'sleeping')\n",
        "      while(checkLimit()['remaining'] < 900):\n",
        "        sleep(60)\n",
        "      print(datetime.now().time().strftime('%H:%M:%S'), 'awake! :)')\n",
        "      \n",
        "    # arrange 100 ids into one list for each request\n",
        "    start_index = i * 100\n",
        "    end_index = min(len(file['tweet_id']), (i+1)*100)\n",
        "    ids = file['tweet_id'][start_index:end_index]\n",
        "\n",
        "    # request extended mode to get full text\n",
        "    tweets = api.statuses_lookup(ids.to_list(), tweet_mode=\"extended\")\n",
        "\n",
        "    # skip the ID if corresponding tweet no longer exists\n",
        "    if tweets != []:\n",
        "      for j in range(len(tweets)):\n",
        "        # fill in dict of info\n",
        "        tweet_info = {}\n",
        "        tweet_info['id'] = tweets[j]._json['id']\n",
        "        if ('retweeted_status' in tweets[j]._json.keys()):\n",
        "          tweet_info['text'] = tweets[j]._json['retweeted_status']['full_text']\n",
        "        else:\n",
        "          tweet_info['text'] = tweets[j]._json['full_text']\n",
        "        tweet_info['user_loc'] = tweets[j]._json['user']['location']\n",
        "        tweet_info['place'] = tweets[j]._json['place']\n",
        "        tweet_info['coords'] = tweets[j]._json['coordinates']\n",
        "        # add row of info to the master df\n",
        "        tweet_df = tweet_df.append(tweet_info, ignore_index=True)\n",
        "    \n",
        "    # increment request count\n",
        "    request_count = request_count + 1\n",
        "  \n",
        "  return tweet_df.set_index('id'), request_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wYLN-w5RqFx"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# HYDRATE CORONAVIS TWEET IDS (by calling hydrateTweetCSV() function)\n",
        "# hydrated_coronavis dict: {yyyy-mm-dd: df}\n",
        "# ------------------------------------------------------------------------------\n",
        "# grab CSV files already hydrated\n",
        "hydrated_files = glob.glob(\"drive/My Drive/Thesis/CoronaVis Hydrated/*.csv\")\n",
        "hydrated_dates = []\n",
        "for f in hydrated_files:\n",
        "  hydrated_dates.append(f[-33:-23])\n",
        "\n",
        "# hydrate files\n",
        "hydrated_coronavis = {} # dict of hydrated tweets\n",
        "for date in sorted(coronavis_ids.keys()):\n",
        "  if date not in hydrated_dates: # ignore dates already hydrated\n",
        "    print(datetime.now().time().strftime('%H:%M:%S'), 'currently hydrating: ', date)\n",
        "    try:\n",
        "      hydrated_coronavis[date], new_request_count = hydrateTweetCSV(coronavis_ids[date], request_count)\n",
        "      request_count = new_request_count\n",
        "      hydrated_coronavis[date].to_csv('{}_coronavis_hydrated.csv'.format(date))\n",
        "      !cp $date\"_coronavis_hydrated.csv\" \"drive/My Drive/Thesis/CoronaVis Hydrated\"\n",
        "      print(datetime.now().time().strftime('%H:%M:%S'), 'finished and saved: ', date)\n",
        "    except Exception as e:\n",
        "      print('ERROR', e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQZYJ6udgSjc"
      },
      "source": [
        "# Example of a given date's data\n",
        "pd.set_option('display.float_format', '{:.0f}'.format)\n",
        "hydrated_coronavis['2020-03-05']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzEeAJLA0KPN"
      },
      "source": [
        "### Twarc hydrated jsonl to CSVs of important info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BORFUPP0ikKu"
      },
      "source": [
        "This is code to read the json lines files in order to save the data into CSV files. For efficiency, won't run this because it'll take way too long-- instead, use this as reference when writing code to handle dates after May 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaeWmeDshfE7"
      },
      "source": [
        "# DO NOT RUN!!!\n",
        "jsonl_files = glob.glob(\"drive/My Drive/Thesis/CoronaVis jsonl/*.jsonl\")\n",
        "for jsonl_file in jsonl_files:\n",
        "  date = jsonl_file[-35:-25]\n",
        "  if date == '2020-05-06':\n",
        "    tweet_df = pd.DataFrame() # data frame to store all important tweet info\n",
        "    print(datetime.now().time().strftime('%H:%M:%S'), 'start: ', date)\n",
        "\n",
        "    with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "        tweet = json.loads(line.rstrip('\\n|\\r'))\n",
        "        # get pertinent info and save to dict\n",
        "        tweet_info = {}\n",
        "        tweet_info['id'] = tweet['id']\n",
        "        if ('retweeted_status' in tweet.keys()):\n",
        "          tweet_info['text'] = tweet['retweeted_status']['full_text']\n",
        "        else:\n",
        "          tweet_info['text'] = tweet['full_text']\n",
        "          tweet_info['user_loc'] = tweet['user']['location']\n",
        "          tweet_info['place'] = tweet['place']\n",
        "          tweet_info['coords'] = tweet['coordinates']\n",
        "        # add tweet info dict to df\n",
        "        tweet_df = tweet_df.append(tweet_info, ignore_index=True)\n",
        "    \n",
        "    tweet_df.set_index('id').to_csv('{}_coronavis_hydrated.csv'.format(date))\n",
        "    !cp $date\"_coronavis_hydrated.csv\" \"drive/My Drive/Thesis/CoronaVis Hydrated\"\n",
        "    print(datetime.now().time().strftime('%H:%M:%S'), 'finished and saved: ', date)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1W30E_WYWew"
      },
      "source": [
        "##CoronaVis: Analysis of Original Number of Tweet IDs and Hydrated Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bByP0YCUgw7"
      },
      "source": [
        "###Do not run (old versions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9az8MzhIYj19"
      },
      "source": [
        "# V1: Count original number of tweet IDs there are to hydrate by date\n",
        "# ran this when i went straight from having run the previous section\n",
        "num_ids = {}\n",
        "for date in sorted(coronavis_ids):\n",
        "  num_ids[date] = len(coronavis_ids[date])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOR8lo-qTquX"
      },
      "source": [
        "# V2: Download original tweet ID files from GDrive and get ID counts\n",
        "# Saved results below so I don't have to run this again\n",
        "coronvis_files = glob.glob(\"drive/My Drive/Thesis/0.CoronaVis Original/*.csv\")\n",
        "num_ids = {}\n",
        "for f in sorted(coronvis_files):\n",
        "  date = f[-14:-4]\n",
        "  num_ids[date] = len(pd.read_csv(f, lineterminator='\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9yj5rYkUj7Q"
      },
      "source": [
        "###Original Tweet ID Counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drcj6jPKPXWW"
      },
      "source": [
        "# Original Tweet ID counts between 03/05/2020 and 12/31/2020\n",
        "num_ids = {'2020-03-05': 4980, '2020-03-06': 40710, '2020-03-07': 46964, '2020-03-08': 66793, '2020-03-09': 72319, '2020-03-10': 91873, '2020-03-11': 160308, '2020-03-12': 363422, '2020-03-13': 207182, '2020-03-14': 255275, '2020-03-15': 170459, '2020-03-16': 118271, '2020-03-17': 163836, '2020-03-18': 177829, '2020-03-19': 145077, '2020-03-20': 177037, '2020-03-21': 118781, '2020-03-22': 138816, '2020-03-23': 130943, '2020-03-24': 114957, '2020-03-25': 96271, '2020-03-26': 57107, '2020-03-27': 83416, '2020-03-28': 77240, '2020-03-29': 81112, '2020-03-30': 62127, '2020-03-31': 65414, '2020-04-01': 71408, '2020-04-02': 19136, '2020-04-03': 49287, '2020-04-04': 35928, '2020-04-05': 40653, '2020-04-06': 52008, '2020-04-07': 44172, '2020-04-08': 50952, '2020-04-09': 60385, '2020-04-10': 207907, '2020-04-11': 215668, '2020-04-12': 197479, '2020-04-13': 222613, '2020-04-14': 138231, '2020-04-15': 133400, '2020-04-16': 177378, '2020-04-17': 196602, '2020-04-18': 234062, '2020-04-19': 214408, '2020-04-20': 193253, '2020-04-21': 234081, '2020-04-22': 242456, '2020-04-23': 136758, '2020-04-24': 224216, '2020-04-25': 198378, '2020-04-26': 181973, '2020-04-27': 150191, '2020-04-28': 146383, '2020-04-29': 227960, '2020-04-30': 196781, '2020-05-01': 199134, '2020-05-02': 196076, '2020-05-03': 202383, '2020-05-04': 218615, '2020-05-05': 205386, '2020-05-06': 227541, '2020-05-07': 92246, '2020-05-08': 208923, '2020-05-09': 205931, '2020-05-10': 190201, '2020-05-11': 225689, '2020-05-12': 213458, '2020-05-13': 124449, '2020-05-14': 209789, '2020-05-15': 199484, '2020-05-16': 170662, '2020-05-17': 186373, '2020-05-18': 184968, '2020-05-19': 175873, '2020-05-20': 210110, '2020-05-21': 133603, '2020-05-22': 196137, '2020-05-23': 197625, '2020-05-24': 192573, '2020-05-25': 196694, '2020-05-26': 187436, '2020-05-27': 116269, '2020-05-28': 195504, '2020-05-29': 176889, '2020-05-30': 174604, '2020-05-31': 268304, '2020-06-01': 171991, '2020-06-02': 163898, '2020-06-03': 138782, '2020-06-04': 197251, '2020-06-05': 143701, '2020-06-06': 151006, '2020-06-07': 154414, '2020-06-08': 137120, '2020-06-09': 166432, '2020-06-10': 100393, '2020-06-11': 176063, '2020-06-12': 207997, '2020-06-13': 115008, '2020-06-14': 169947, '2020-06-15': 260678, '2020-06-16': 206380, '2020-06-17': 198091, '2020-06-18': 169317, '2020-06-19': 217111, '2020-06-20': 238231, '2020-06-21': 225608, '2020-06-22': 148628, '2020-06-23': 272976, '2020-06-24': 151913, '2020-06-25': 282053, '2020-06-26': 191681, '2020-06-27': 242723, '2020-06-28': 300385, '2020-06-29': 289477, '2020-06-30': 172250, '2020-07-01': 274578, '2020-07-02': 218013, '2020-07-03': 180861, '2020-07-04': 240717, '2020-07-05': 250761, '2020-07-06': 223667, '2020-07-07': 174113, '2020-07-08': 172032, '2020-07-09': 243205, '2020-07-10': 298545, '2020-07-11': 280284, '2020-07-12': 268579, '2020-07-13': 289371, '2020-07-14': 274713, '2020-07-15': 329245, '2020-07-16': 290532, '2020-07-17': 298164, '2020-07-18': 254959, '2020-07-19': 245136, '2020-07-20': 205134, '2020-07-21': 181486, '2020-07-22': 251168, '2020-07-23': 104637, '2020-07-24': 177565, '2020-07-25': 227131, '2020-07-26': 213809, '2020-07-27': 150773, '2020-07-28': 336992, '2020-07-29': 308633, '2020-07-30': 339237, '2020-07-31': 281021, '2020-08-01': 252411, '2020-08-02': 253018, '2020-08-03': 205264, '2020-08-04': 149669, '2020-08-05': 143159, '2020-08-06': 206359, '2020-08-07': 102994, '2020-08-08': 171702, '2020-08-09': 211208, '2020-08-10': 156467, '2020-08-11': 146740, '2020-08-12': 188518, '2020-08-13': 88712, '2020-08-14': 299641, '2020-08-15': 155255, '2020-08-16': 149400, '2020-08-17': 184192, '2020-08-18': 137470, '2020-08-19': 122625, '2020-08-20': 79571, '2020-08-21': 132712, '2020-08-22': 160069, '2020-08-23': 161538, '2020-08-24': 176354, '2020-08-25': 130600, '2020-08-26': 47408, '2020-08-27': 212956, '2020-08-28': 197051, '2020-08-29': 175569, '2020-08-30': 170935, '2020-08-31': 131129, '2020-09-01': 139931, '2020-09-02': 25702, '2020-09-03': 212063, '2020-09-04': 191834, '2020-09-05': 131617, '2020-09-06': 128668, '2020-09-07': 167433, '2020-09-08': 51466, '2020-09-09': 152867, '2020-09-10': 249385, '2020-09-11': 226722, '2020-09-12': 190810, '2020-09-13': 183199, '2020-09-14': 194086, '2020-09-15': 145348, '2020-09-16': 246209, '2020-09-17': 181966, '2020-09-18': 70519, '2020-09-19': 97464, '2020-09-20': 118595, '2020-09-21': 126991, '2020-09-22': 197568, '2020-09-23': 217557, '2020-09-24': 189697, '2020-09-25': 127658, '2020-09-26': 140304, '2020-09-27': 115098, '2020-09-28': 122249, '2020-09-29': 157275, '2020-09-30': 184314, '2020-10-01': 165711, '2020-10-02': 510496, '2020-10-03': 552279, '2020-10-04': 451207, '2020-10-05': 482118, '2020-10-06': 531752, '2020-10-07': 427239, '2020-10-08': 295314, '2020-10-09': 174588, '2020-10-10': 189328, '2020-10-11': 256890, '2020-10-12': 195114, '2020-10-13': 224906, '2020-10-14': 247190, '2020-10-15': 185716, '2020-10-16': 154491, '2020-10-17': 173839, '2020-10-18': 187955, '2020-10-19': 252184, '2020-10-20': 371880, '2020-10-21': 373400, '2020-10-22': 54100, '2020-10-23': 270614, '2020-10-24': 411440, '2020-10-25': 464640, '2020-10-26': 334652, '2020-10-27': 325088, '2020-10-28': 139962, '2020-10-29': 185497, '2020-10-30': 200514, '2020-10-31': 235570, '2020-11-01': 196851, '2020-11-02': 204186, '2020-11-03': 164919, '2020-11-04': 139287, '2020-11-05': 218949, '2020-11-06': 168663, '2020-11-07': 248474, '2020-11-08': 173409, '2020-11-09': 250201, '2020-11-10': 144560, '2020-11-11': 106632, '2020-11-12': 12695, '2020-11-13': 246225, '2020-11-14': 130899, '2020-11-15': 165042, '2020-11-16': 178315, '2020-11-17': 292196, '2020-11-18': 122639, '2020-11-19': 89009, '2020-11-20': 201452, '2020-11-21': 254858, '2020-11-22': 211376, '2020-11-23': 164649, '2020-11-24': 92873, '2020-11-25': 235326, '2020-11-26': 190102, '2020-11-27': 158432, '2020-11-28': 166272, '2020-11-29': 179204, '2020-11-30': 110272, '2020-12-01': 98095, '2020-12-02': 194099, '2020-12-03': 248612, '2020-12-04': 235511, '2020-12-05': 207024, '2020-12-06': 197600, '2020-12-07': 117571, '2020-12-08': 20342, '2020-12-09': 246537, '2020-12-10': 239066, '2020-12-11': 216218, '2020-12-12': 172376, '2020-12-13': 172819, '2020-12-14': 162497, '2020-12-16': 50353, '2020-12-17': 153189, '2020-12-18': 270150, '2020-12-19': 231315, '2020-12-20': 234397, '2020-12-21': 302219, '2020-12-22': 289477, '2020-12-23': 224399, '2020-12-24': 190456, '2020-12-25': 114224, '2020-12-26': 36457, '2020-12-27': 149243, '2020-12-28': 257590, '2020-12-29': 235148, '2020-12-30': 286807, '2020-12-31': 223279}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOHEjFn3JIzr"
      },
      "source": [
        "num_ids_df = pd.DataFrame(num_ids.items())\n",
        "num_ids_df.columns = ['date', 'tweet_count']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOQJt4cXJjRB"
      },
      "source": [
        "num_ids_df[210:240]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JkI40J-GWUo"
      },
      "source": [
        "# find where to place ticks  (start of each month) and set their labels\n",
        "month_ticks = ['Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "tick_indices = []\n",
        "\n",
        "curr_month = ''\n",
        "for i, date in enumerate(num_ids.keys()):\n",
        "  month = date[5:7]\n",
        "  if month != curr_month:\n",
        "    tick_indices.append(i)\n",
        "    curr_month = month"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHal6CqkD1pn"
      },
      "source": [
        "# Title: Number of CoronaVis Tweet IDs Available Per Day\n",
        "# peak on oct 3 - president trump and first lady melania trump tested positive \n",
        "# for COVID the day before. this peak is likely a reaction to it\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=num_ids_df,\n",
        "    x='date', y='tweet_count', \n",
        "    kind=\"line\", height=15, aspect=2.5\n",
        ").set_axis_labels(\"Date (Month in 2020)\", \"Number of Tweet IDs\", labelpad=35)\n",
        "g.set(ylim=(0, 575000))\n",
        "g.set(xticks=tick_indices)\n",
        "g.set_xticklabels(month_ticks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY5C3ZFvrvj-"
      },
      "source": [
        "# Plot results\n",
        "# no longer used, for seaborn visualization above\n",
        "x,y = zip(*num_ids.items())\n",
        "rotate, ax = plt.subplots(figsize=(75,8))\n",
        "ax.set_title('Number of Tweet IDs per day')\n",
        "plt.xticks(rotation=90)\n",
        "plt.plot(x, y);\n",
        "# peak on oct 3 - president trump and first lady melania trump tested positive \n",
        "# for COVID the day before. this peak is likely a reaction to it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFe81WnQrgEM"
      },
      "source": [
        "# Number of tweet ids queried for hydration:\n",
        "total_colab = 0 # count of tweet ids queried using above code\n",
        "total_twarc = 0 # count of tweet ids queried using twarc\n",
        "for date in sorted(num_ids):\n",
        "  if date < '2020-05-06':\n",
        "    total_colab += num_ids[date]\n",
        "  else:\n",
        "    total_twarc += num_ids[date]\n",
        "print('total queried using colab:', total_colab)\n",
        "print('total queried using twarc: ', total_twarc)\n",
        "print('total queried:', total_colab + total_twarc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH-tkyCOUrhM"
      },
      "source": [
        "###Do not run (old code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX_Fkv_0z0Ba"
      },
      "source": [
        "# Count number of tweets successfully hydrated using colab\n",
        "coronavis_hydrated_files = glob.glob(\"drive/My Drive/Thesis/1a.CoronaVis Hydrated/*.csv\")\n",
        "coronavis_tweets = {}\n",
        "num_tweets = {}\n",
        "for f in sorted(coronavis_hydrated_files):\n",
        "  date = f[-33:-23]\n",
        "  coronavis_tweets[date] = pd.read_csv(f, lineterminator='\\n')\n",
        "  num_tweets[date] = len(coronavis_tweets[date])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amyw06fYnK1A"
      },
      "source": [
        "# DO NOT RUN!!! I just ran it in my laptop's terminal and it was significantly faster\n",
        "# Count number of tweets successfully hydrated using twarc\n",
        "# coronavis_hydrated_files_2 = glob.glob(\"drive/My Drive/Thesis/CoronaVis jsonl/*.jsonl\")\n",
        "# print(datetime.now().time().strftime('%H:%M:%S'), 'start')\n",
        "# for f in sorted(coronavis_hydrated_files_2):\n",
        "#   date = f[-35:-25] #double check dates before fully running\n",
        "#   num_tweets[date] = subprocess.check_output(['wc', '-l', f]).decode(\"utf-8\").split()[0]\n",
        "# print(datetime.now().time().strftime('%H:%M:%S'), 'end')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQWoEkUg_vRl"
      },
      "source": [
        "# Tweet counts for 05/05 to 08/07, found using a shell script locally\n",
        "daily_counts = [194138,  78221, 180063, 175684, 159459, 193267, 181988, 107274, 180345, 169278, 141762, 157660, 159126, 152434, 180419, 113446, 168129, 168755, 162165, 165694, 161429, 100477, 166631, 151054, 148664, 227103, 145312, 141767, 118005, 169786, 120088, 128830, 129913, 118268, 145074,  86053, 152540, 180517,  98611, 148302, 227485, 178975, 172096, 147349, 191960, 208625, 193122, 126727, 238324, 134315, 248471, 167144, 209088, 263783, 249119, 139936, 236912, 185609, 156744, 200357, 215558, 197596, 144650, 150234, 214605, 256758, 246835, 227680, 254347, 231004, 277297, 248688, 254110, 211511, 209360, 182971, 161771, 215823,  92191, 156151, 198849, 188822, 131231, 276892, 271086, 298734, 248691, 223718, 223515, 182846, 131796, 126342, 179134,  84392, 124148, 158325, 117741, 107093, 145354, 66790, 154444, 114484, 103892, 135835, 101112, 95036, 60739, 99370, 117358, 117981, 129937, 98044, 36857, 170240, 159679, 133874, 105210, 84796, 102616, 19631, 162955, 146549, 93257, 89652, 121356, 36922, 117827, 193334, 179803, 152254, 147634, 146124, 110707, 190407, 132233, 52306, 76762, 89633, 99335, 161165, 176233, 148991, 96869, 106853, 87595, 92033, 121104, 148997, 133100, 379026, 436282, 351184, 379817, 421511, 338279, 236816, 142917, 156889, 199151, 158029, 181063, 197712, 151932, 122393, 135731, 145412, 125424, 154964, 149130, 21164, 106903, 163843, 194218, 131788, 185754, 108401, 143916, 154400, 189231, 147902, 159463, 132584, 109027, 172654, 134046, 201783, 137350, 198107, 117073, 87193, 9359, 189160, 103914, 135074, 144441, 233598, 98005, 67618, 163859, 196601, 156792, 126540, 74520, 187570, 154747, 125643, 130822, 142522, 81579, 81744, 159790, 200054, 187236, 160154, 158647, 88053, 16649, 193176, 194171, 176388, 140498, 133113, 132287, 42001, 127344, 214756, 183178, 183070, 225058, 212982, 163172, 150637, 89401, 27306, 108480, 179675, 181734, 223107, 181383]\n",
        "index = 0\n",
        "for date in sorted(num_ids):\n",
        "  if date >= '2020-05-06':\n",
        "    num_tweets[date] = daily_counts[index]\n",
        "    index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs_7XceCU_vz"
      },
      "source": [
        "###Successfully hydrated counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vKEiXI0UBhV"
      },
      "source": [
        "num_tweets = {'2020-03-05': 3877, '2020-03-06': 31631, '2020-03-07': 35996, '2020-03-08': 51948, '2020-03-09': 52907, '2020-03-10': 65964, '2020-03-11': 119743, '2020-03-12': 267224, '2020-03-13': 153734, '2020-03-14': 195051, '2020-03-15': 127096, '2020-03-16': 85877, '2020-03-17': 126618, '2020-03-18': 129069, '2020-03-19': 107279, '2020-03-20': 120273, '2020-03-21': 88308, '2020-03-22': 107499, '2020-03-23': 102452, '2020-03-24': 84013, '2020-03-25': 71704, '2020-03-26': 44882, '2020-03-27': 65392, '2020-03-28': 56889, '2020-03-29': 60849, '2020-03-30': 46711, '2020-03-31': 47094, '2020-04-01': 53635, '2020-04-02': 13233, '2020-04-03': 37564, '2020-04-04': 27916, '2020-04-05': 31443, '2020-04-06': 39426, '2020-04-07': 34176, '2020-04-08': 37942, '2020-04-09': 47794, '2020-04-10': 172455, '2020-04-11': 177722, '2020-04-12': 162233, '2020-04-13': 188077, '2020-04-14': 117494, '2020-04-15': 113315, '2020-04-16': 148096, '2020-04-17': 167109, '2020-04-18': 186477, '2020-04-19': 175927, '2020-04-20': 162161, '2020-04-21': 198436, '2020-04-22': 203050, '2020-04-23': 114947, '2020-04-24': 188628, '2020-04-25': 163942, '2020-04-26': 152573, '2020-04-27': 124779, '2020-04-28': 125296, '2020-04-29': 191269, '2020-04-30': 166209, '2020-05-01': 170201, '2020-05-02': 164906, '2020-05-03': 167868, '2020-05-04': 186756, '2020-05-05': 153462, '2020-05-06': 194138, '2020-05-07': 78221, '2020-05-08': 180063, '2020-05-09': 175684, '2020-05-10': 159459, '2020-05-11': 193267, '2020-05-12': 181988, '2020-05-13': 107274, '2020-05-14': 180345, '2020-05-15': 169278, '2020-05-16': 141762, '2020-05-17': 157660, '2020-05-18': 159126, '2020-05-19': 152434, '2020-05-20': 180419, '2020-05-21': 113446, '2020-05-22': 168129, '2020-05-23': 168755, '2020-05-24': 162165, '2020-05-25': 165694, '2020-05-26': 161429, '2020-05-27': 100477, '2020-05-28': 166631, '2020-05-29': 151054, '2020-05-30': 148664, '2020-05-31': 227103, '2020-06-01': 145312, '2020-06-02': 141767, '2020-06-03': 118005, '2020-06-04': 169786, '2020-06-05': 120088, '2020-06-06': 128830, '2020-06-07': 129913, '2020-06-08': 118268, '2020-06-09': 145074, '2020-06-10': 86053, '2020-06-11': 152540, '2020-06-12': 180517, '2020-06-13': 98611, '2020-06-14': 148302, '2020-06-15': 227485, '2020-06-16': 178975, '2020-06-17': 172096, '2020-06-18': 147349, '2020-06-19': 191960, '2020-06-20': 208625, '2020-06-21': 193122, '2020-06-22': 126727, '2020-06-23': 238324, '2020-06-24': 134315, '2020-06-25': 248471, '2020-06-26': 167144, '2020-06-27': 209088, '2020-06-28': 263783, '2020-06-29': 249119, '2020-06-30': 139936, '2020-07-01': 236912, '2020-07-02': 185609, '2020-07-03': 156744, '2020-07-04': 200357, '2020-07-05': 215558, '2020-07-06': 197596, '2020-07-07': 144650, '2020-07-08': 150234, '2020-07-09': 214605, '2020-07-10': 256758, '2020-07-11': 246835, '2020-07-12': 227680, '2020-07-13': 254347, '2020-07-14': 231004, '2020-07-15': 277297, '2020-07-16': 248688, '2020-07-17': 254110, '2020-07-18': 211511, '2020-07-19': 209360, '2020-07-20': 182971, '2020-07-21': 161771, '2020-07-22': 215823, '2020-07-23': 92191, '2020-07-24': 156151, '2020-07-25': 198849, '2020-07-26': 188822, '2020-07-27': 131231, '2020-07-28': 276892, '2020-07-29': 271086, '2020-07-30': 298734, '2020-07-31': 248691, '2020-08-01': 223718, '2020-08-02': 223515, '2020-08-03': 182846, '2020-08-04': 131796, '2020-08-05': 126342, '2020-08-06': 179134, '2020-08-07': 84392, '2020-08-08': 124148, '2020-08-09': 158325, '2020-08-10': 117741, '2020-08-11': 107093, '2020-08-12': 145354, '2020-08-13': 66790, '2020-08-14': 154444, '2020-08-15': 114484, '2020-08-16': 103892, '2020-08-17': 135835, '2020-08-18': 101112, '2020-08-19': 95036, '2020-08-20': 60739, '2020-08-21': 99370, '2020-08-22': 117358, '2020-08-23': 117981, '2020-08-24': 129937, '2020-08-25': 98044, '2020-08-26': 36857, '2020-08-27': 170240, '2020-08-28': 159679, '2020-08-29': 133874, '2020-08-30': 105210, '2020-08-31': 84796, '2020-09-01': 102616, '2020-09-02': 19631, '2020-09-03': 162955, '2020-09-04': 146549, '2020-09-05': 93257, '2020-09-06': 89652, '2020-09-07': 121356, '2020-09-08': 36922, '2020-09-09': 117827, '2020-09-10': 193334, '2020-09-11': 179803, '2020-09-12': 152254, '2020-09-13': 147634, '2020-09-14': 146124, '2020-09-15': 110707, '2020-09-16': 190407, '2020-09-17': 132233, '2020-09-18': 52306, '2020-09-19': 76762, '2020-09-20': 89633, '2020-09-21': 99335, '2020-09-22': 161165, '2020-09-23': 176233, '2020-09-24': 148991, '2020-09-25': 96869, '2020-09-26': 106853, '2020-09-27': 87595, '2020-09-28': 92033, '2020-09-29': 121104, '2020-09-30': 148997, '2020-10-01': 133100, '2020-10-02': 379026, '2020-10-03': 436282, '2020-10-04': 351184, '2020-10-05': 379817, '2020-10-06': 421511, '2020-10-07': 338279, '2020-10-08': 236816, '2020-10-09': 142917, '2020-10-10': 156889, '2020-10-11': 199151, '2020-10-12': 158029, '2020-10-13': 181063, '2020-10-14': 197712, '2020-10-15': 151932, '2020-10-16': 122393, '2020-10-17': 135731, '2020-10-18': 145412, '2020-10-19': 125424, '2020-10-20': 154964, '2020-10-21': 149130, '2020-10-22': 21164, '2020-10-23': 106903, '2020-10-24': 163843, '2020-10-25': 194218, '2020-10-26': 131788, '2020-10-27': 185754, '2020-10-28': 108401, '2020-10-29': 143916, '2020-10-30': 154400, '2020-10-31': 189231, '2020-11-01': 147902, '2020-11-02': 159463, '2020-11-03': 132584, '2020-11-04': 109027, '2020-11-05': 172654, '2020-11-06': 134046, '2020-11-07': 201783, '2020-11-08': 137350, '2020-11-09': 198107, '2020-11-10': 117073, '2020-11-11': 87193, '2020-11-12': 9359, '2020-11-13': 189160, '2020-11-14': 103914, '2020-11-15': 135074, '2020-11-16': 144441, '2020-11-17': 233598, '2020-11-18': 98005, '2020-11-19': 67618, '2020-11-20': 163859, '2020-11-21': 196601, '2020-11-22': 156792, '2020-11-23': 126540, '2020-11-24': 74520, '2020-11-25': 187570, '2020-11-26': 154747, '2020-11-27': 125643, '2020-11-28': 130822, '2020-11-29': 142522, '2020-11-30': 81579, '2020-12-01': 81744, '2020-12-02': 159790, '2020-12-03': 200054, '2020-12-04': 187236, '2020-12-05': 160154, '2020-12-06': 158647, '2020-12-07': 88053, '2020-12-08': 16649, '2020-12-09': 193176, '2020-12-10': 194171, '2020-12-11': 176388, '2020-12-12': 140498, '2020-12-13': 133113, '2020-12-14': 132287, '2020-12-16': 42001, '2020-12-17': 127344, '2020-12-18': 214756, '2020-12-19': 183178, '2020-12-20': 183070, '2020-12-21': 225058, '2020-12-22': 212982, '2020-12-23': 163172, '2020-12-24': 150637, '2020-12-25': 89401, '2020-12-26': 27306, '2020-12-27': 108480, '2020-12-28': 179675, '2020-12-29': 181734, '2020-12-30': 223107, '2020-12-31': 181383}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwUJLHuMQN77"
      },
      "source": [
        "num_tweets_df = pd.DataFrame(num_tweets.items())\n",
        "num_tweets_df.columns = ['date', 'tweet_count']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTIL46TJQeNQ"
      },
      "source": [
        "# Title: Number of CoronaVis Tweet IDs Successfully Hydrated Per Day\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=num_tweets_df,\n",
        "    x='date', y='tweet_count', \n",
        "    kind=\"line\", height=15, aspect=2.5\n",
        ").set_axis_labels(\"Date (Month in 2020)\", \"Number of Hydrated Tweets\", labelpad=35)\n",
        "g.set(ylim=(0, 575000))\n",
        "g.set(xticks=tick_indices) # created in original tweet ID counts graphic\n",
        "g.set_xticklabels(month_ticks) # ditto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUzgxaCJ1xUs"
      },
      "source": [
        "# visualize number of tweets successfully hydrated\n",
        "# no longer used, see seaborn visualization above\n",
        "x,y = zip(*num_tweets.items())\n",
        "rotate, ax = plt.subplots(figsize=(75,8))\n",
        "ax.set_title('Number of Tweets Successfully Hydrated per day')\n",
        "plt.xticks(rotation=90)\n",
        "plt.plot(x, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPDb-fDE1lqX"
      },
      "source": [
        "# sum of tweets successfully found so far\n",
        "total_hydrated_tweepy = 0\n",
        "total_hydrated_twarc = 0\n",
        "for date in sorted(num_tweets):\n",
        "  if date < '2020-05-06':\n",
        "    total_hydrated_tweepy += num_tweets[date]\n",
        "  else:\n",
        "    total_hydrated_twarc += num_tweets[date]\n",
        "\n",
        "print('total queried using tweepy:', total_hydrated_tweepy)\n",
        "print('total queried using twarc: ', total_hydrated_twarc)\n",
        "print('total queried:', total_hydrated_tweepy + total_hydrated_twarc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHrUHG6Z2Eal"
      },
      "source": [
        "# calculate percent of tweet ids successfully hydrated per day\n",
        "percent_hydrated = {}\n",
        "for date in sorted(num_tweets):\n",
        "  percent_hydrated[date] = num_tweets[date] / num_ids[date]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NJpaVVEUZn5"
      },
      "source": [
        "percent_hydrated_df = pd.DataFrame(percent_hydrated.items())\n",
        "percent_hydrated_df.columns = ['date', 'decimal']\n",
        "percent_hydrated_df['percent'] = percent_hydrated_df.apply(lambda row: row.decimal * 100, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsxaPa8uUwmf"
      },
      "source": [
        "# Title: Percent of CoronaVis Tweet IDs Successfully Hydrated Per Day\n",
        "# Note dip starting from the second collection of tweets by CoronaVis\n",
        "# Can't find a real reason for that huge dip in later October?\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=percent_hydrated_df,\n",
        "    x='date', y='percent', \n",
        "    kind=\"line\", height=15, aspect=2.5\n",
        ").set_axis_labels(\"Date (Month in 2020)\", \"Percent (%)\", labelpad=35)\n",
        "g.set(ylim=(30, 100))\n",
        "g.set(xticks=tick_indices) # created in original tweet ID counts graphic\n",
        "g.set_xticklabels(month_ticks) # ditto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBJtxtye2WD2"
      },
      "source": [
        "# visualize percent of tweet ids successfully hydrated per day\n",
        "# # no longer used, see seaborn visualization above\n",
        "x,y = zip(*percent_hydrated.items())\n",
        "rotate, ax = plt.subplots(figsize=(75,8))\n",
        "ax.set_title('Percentage of Tweet IDs Successfully Hydrated per day')\n",
        "plt.xticks(rotation=90)\n",
        "plt.plot(x, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0I9kkGWmLIV"
      },
      "source": [
        "## 2. CoronaVis: State Location Discovery and Sorting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRkIusfSJNCj"
      },
      "source": [
        "###Part One: 03/05/2020 to 05/05/2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7tQBnbXiktI"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# GET HYDRATED TWEET CSVs FROM GDRIVE (03-05 to 05-05)\n",
        "# hydrated_dict: {yyyy-mm-dd: tweets}\n",
        "# ------------------------------------------------------------------------------\n",
        "hydrated_files = glob.glob(\"drive/My Drive/Thesis/CoronaVis Hydrated/*.csv\")\n",
        "hydrated_dict = {}\n",
        "for f in hydrated_files:\n",
        "  date = f[-33:-23]\n",
        "  hydrated_dict[date] = pd.read_csv(f, lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkiBEoXGSqB3"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# CHECK FOR STATE NAME OR ABBREVIATION IN STRING\n",
        "# ------------------------------------------------------------------------------\n",
        "def check_for_state(loc):\n",
        "  loc = loc.lower()\n",
        "  # check for state name\n",
        "  for state in states_abbrev:\n",
        "    if state in loc:\n",
        "      return states_abbrev[state]\n",
        "  # check for state abbreviations\n",
        "  for state in states_lower:\n",
        "    # can't just look for the two letters, they must stand alone\n",
        "    abbrev_regex = re.compile(r'\\b[,]*{}[,]*\\b'.format(state))\n",
        "    result = re.search(abbrev_regex, loc)\n",
        "    if (result):\n",
        "      return state\n",
        "  # no state found\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPXJuFxeXFZw"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION TO QUERY GEOCODE\n",
        "# source: https://stackoverflow.com/questions/60083187/python-geopy-nominatim-too-many-requests\n",
        "# not currently used because takes too long lmao and I have enough tweets\n",
        "# ------------------------------------------------------------------------------\n",
        "def geocode_func(geolocator, loc, sleep_sec):\n",
        "  try:\n",
        "    return geolocator.geocode(loc)\n",
        "  except GeocoderTimedOut:\n",
        "    print('TIMED OUT: GeocoderTimedOut: Retrying...')\n",
        "    sleep(randint(1*100,sleep_sec*100)/100)\n",
        "    return geocode_func(geolocator, loc, sleep_sec)\n",
        "  except GeocoderServiceError as e:\n",
        "    print('CONNECTION REFUSED: GeocoderServiceError encountered.')\n",
        "    return None\n",
        "  except Exception as e:\n",
        "    print('ERROR: Terminating due to exception {}'.format(e))\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0RVrgeWjXbu"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# GEOPARSING: separate tweets into DFs by state\n",
        "# VERSION 1: TAKES IN DF FOR A DATE, WHERE EACH ROW IS A TWEET\n",
        "# ------------------------------------------------------------------------------\n",
        "def find_state_locations(tweets, date):\n",
        "  # initialize state lists\n",
        "  state_dict = {}\n",
        "  for state in states_lower:\n",
        "    state_dict[state] = []\n",
        "\n",
        "  # counters (for reference of how many tweets were retained)\n",
        "  success = 0\n",
        "  unqueried = 0\n",
        "  invalid = 0\n",
        "\n",
        "  # for each tweet entry in the date's DF\n",
        "  for index, row in tqdm(tweets.iterrows(), total=tweets.shape[0], desc=date):\n",
        "    tweet = {'date': date, 'id': row['id'], 'text': row['text']}\n",
        "\n",
        "    # CHECK PLACE FIELD IF IT EXISTS\n",
        "    if row['place'] == row['place']:\n",
        "      place = ast.literal_eval(row['place'])\n",
        "      if place['country_code']=='US': # place in the US\n",
        "        place_name = place['full_name'].lower()\n",
        "        if (place_name[-2:] in states_lower): # state abbrev is in place\n",
        "          state_dict[place_name[-2:]].append(tweet)\n",
        "          success += 1\n",
        "        elif place_name[:-5] in states_abbrev: # state name is in place\n",
        "          abbrev = states_abbrev[place_name[:-5]]\n",
        "          state_dict[abbrev].append(tweet)\n",
        "          success += 1\n",
        "        else: # search geocode for location\n",
        "          unqueried += 1\n",
        "      else:\n",
        "        invalid += 1\n",
        "\n",
        "    # CHECK USER_LOC FIELD IF IT EXISTS\n",
        "    elif row['user_loc'] == row['user_loc']: \n",
        "      state = check_for_state(row['user_loc'])\n",
        "      if state != None:\n",
        "        state_dict[state].append(tweet)\n",
        "        success += 1\n",
        "      else:\n",
        "        unqueried+=1\n",
        "\n",
        "    else:\n",
        "      unqueried += 1\n",
        "    \n",
        "  # return state_dict\n",
        "  return state_dict, success, unqueried, invalid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW6UvX7ecxvp"
      },
      "source": [
        "# # for purpose of test runs\n",
        "# f = \"drive/My Drive/Thesis/CoronaVis Hydrated/2020-04-18_coronavis_hydrated.csv\"\n",
        "# test = pd.read_csv(f, lineterminator='\\n')\n",
        "# state_dict, success, unqueried, invalid = find_state_locations(test, '2020-04-18')\n",
        "# print('success: ', success, 'unqueried: ', unqueried, 'invalid: ', invalid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB5k_-anOlMh"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# SET UP FOR ROUND 1 OF LOCATION FINDING: 03/05-05/05\n",
        "# ------------------------------------------------------------------------------\n",
        "tweets_by_state = {} # master df of tweets by state\n",
        "for state in states_lower:\n",
        "  tweets_by_state[state] = pd.DataFrame()\n",
        "\n",
        "counters = pd.DataFrame() # counters for how many tweets were retained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPO2jL7NV6_a"
      },
      "source": [
        "# run just 2020-05-05 to double check the numbers-- not sure why so many are unqueried here?\n",
        "may5_file = \"drive/My Drive/Thesis/1a.CoronaVis Hydrated/2020-05-05_coronavis_hydrated.csv\"\n",
        "may5 = pd.read_csv(may5_file, lineterminator='\\n')\n",
        "\n",
        "new_state_dict, success, unqueried, invalid = find_state_locations(may5, '2020-05-05')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTzIGatKWv52"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# ROUND 1 OF LOCATION FINDING: 03/05-05/05\n",
        "# ------------------------------------------------------------------------------\n",
        "# Run for all dates in range\n",
        "for date in hydrated_dict:\n",
        "  new_state_dict, success, unqueried, invalid = find_state_locations(hydrated_dict[date], date)\n",
        "  # update state df\n",
        "  for state in states_lower:\n",
        "    new_df = pd.DataFrame(new_state_dict[state], columns=['date', 'id', 'text'])\n",
        "    tweets_by_state[state] = pd.concat([tweets_by_state[state], new_df])\n",
        "  # update counters df\n",
        "  total = success + unqueried + invalid\n",
        "  count = {'date': date, 'success': success, 'unqueried': unqueried, 'invalid': invalid, 'total': total}\n",
        "  counters = counters.append(count, ignore_index=True)\n",
        "\n",
        "# Update tweets in GDrive\n",
        "for state in states_lower:\n",
        "  state_df = tweets_by_state[state]\n",
        "  state_df.to_csv('{}_tweets.csv'.format(state))\n",
        "  !cp $state\"_tweets.csv\" \"drive/My Drive/Thesis/By Location\"\n",
        "\n",
        "# Update counters in GDrive\n",
        "counters = counters.set_index('date')\n",
        "counters.to_csv('tweet_counters.csv')\n",
        "!cp \"tweet_counters.csv\" \"drive/My Drive/Thesis/By Location\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uv3x3yZJZ0l"
      },
      "source": [
        "### Part Two: 05/06/2020 to 08/07/2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LEdMwT0yNMd"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# GEOPARSING: separate tweets into DFs by state\n",
        "# VERSION 2: takes in jsonl file for a date, where each json entry is a tweet\n",
        "# Used by parts 2 and 3\n",
        "# ------------------------------------------------------------------------------\n",
        "def find_state_locations_jsonl(jsonl_file, date, total_count):\n",
        "  # initialize state lists\n",
        "  state_dict = {}\n",
        "  for state in states_lower:\n",
        "    state_dict[state] = []\n",
        "\n",
        "  # counters (for reference of how many tweets were retained)\n",
        "  success = 0\n",
        "  unqueried = 0\n",
        "  invalid = 0\n",
        "\n",
        "  # for each tweet json entry in the date's jsonl file\n",
        "  with jsonlines.open(jsonl_file) as reader:\n",
        "    for obj in tqdm(reader, total=total_count, desc=date):\n",
        "      # get pertinent tweet info and save to dict\n",
        "      tweet = {}\n",
        "      tweet['date'] = date\n",
        "      tweet['id'] = obj['id']\n",
        "      if ('retweeted_status' in obj.keys()):\n",
        "        tweet['text'] = obj['retweeted_status']['full_text']\n",
        "      else:\n",
        "        tweet['text'] = obj['full_text']\n",
        "      # get location info\n",
        "      user_loc = obj['user']['location']\n",
        "      place = obj['place']\n",
        "\n",
        "      # CHECK PLACE FIELD IF IT EXISTS\n",
        "      if place is not None:\n",
        "        if place['country_code']=='US': # place in the US\n",
        "          place_name = place['full_name'].lower()\n",
        "          if (place_name[-2:] in states_lower): # state abbrev is in place\n",
        "            state_dict[place_name[-2:]].append(tweet)\n",
        "            success += 1\n",
        "          elif place_name[:-5] in states_abbrev: # state name is in place\n",
        "            abbrev = states_abbrev[place_name[:-5]]\n",
        "            state_dict[abbrev].append(tweet)\n",
        "            success += 1\n",
        "          else: # search geocode for location\n",
        "            unqueried += 1\n",
        "        else:\n",
        "          invalid += 1\n",
        "\n",
        "      # CHECK USER_LOC FIELD IF IT EXISTS\n",
        "      elif user_loc == user_loc: \n",
        "        state = check_for_state(user_loc)\n",
        "        if state != None:\n",
        "          state_dict[state].append(tweet)\n",
        "          success += 1\n",
        "        else:\n",
        "          unqueried+=1\n",
        "\n",
        "      else:\n",
        "        unqueried += 1\n",
        "    \n",
        "  # return state_dict\n",
        "  return state_dict, success, unqueried, invalid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2A5UrIBOhkF"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# SET UP FOR ROUND 2 OF LOCATION FINDING: 05/06-08/07\n",
        "# Updated to just use lists and later convert to df for speed\n",
        "# ------------------------------------------------------------------------------\n",
        "tweets_by_state_2 = {} # master df of tweets by state\n",
        "for state in states_lower:tweets_by_state_2[state] = []\n",
        "\n",
        "counters_2 = [] # counters for how many tweets were retained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX0d-yfAfP7V"
      },
      "source": [
        "# # single test example\n",
        "# f = \"drive/My Drive/Thesis/CoronaVis jsonl/2020-05-06_coronavis_hydrated.jsonl\"\n",
        "# date = '2020-05-06'\n",
        "# new_state_dict, success, unqueried, invalid = find_state_locations_jsonl(f, date, 194138)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ovcPiOrF8Q"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# ROUND 2 OF LOCATION FINDING: 05/06-08/07\n",
        "# ------------------------------------------------------------------------------\n",
        "# tweet counts for each\n",
        "daily_counts = [194138,  78221, 180063, 175684, 159459, 193267, 181988, 107274, 180345, 169278, 141762, 157660, 159126, 152434, 180419, 113446, 168129, 168755, 162165, 165694, 161429, 100477, 166631, 151054, 148664, 227103, 145312, 141767, 118005, 169786, 120088, 128830, 129913, 118268, 145074,  86053, 152540, 180517,  98611, 148302, 227485, 178975, 172096, 147349, 191960, 208625, 193122, 126727, 238324, 134315, 248471, 167144, 209088, 263783, 249119, 139936, 236912, 185609, 156744, 200357, 215558, 197596, 144650, 150234, 214605, 256758, 246835, 227680, 254347, 231004, 277297, 248688, 254110, 211511, 209360, 182971, 161771, 215823,  92191, 156151, 198849, 188822, 131231, 276892, 271086, 298734, 248691, 223718, 223515, 182846, 131796, 126342, 179134,  84392]\n",
        "index = 0\n",
        "\n",
        "# Run for all dates in range\n",
        "jsonl_files = glob.glob(\"drive/My Drive/Thesis/CoronaVis jsonl/*.jsonl\")\n",
        "for jsonl_file in sorted(jsonl_files):\n",
        "  date = jsonl_file[-35:-25]\n",
        "  new_state_dict, success, unqueried, invalid = find_state_locations_jsonl(jsonl_file, date, daily_counts[index])\n",
        "  index += 1\n",
        "  # update state df\n",
        "  for state in states_lower:\n",
        "    tweets_by_state_2[state] = tweets_by_state_2[state] + new_state_dict[state]\n",
        "  # update counters df\n",
        "  total = success + unqueried + invalid\n",
        "  count = {'date': date, 'success': success, 'unqueried': unqueried, 'invalid': invalid, 'total': total}\n",
        "  counters_2.append(count)\n",
        "\n",
        "# Update tweets in GDrive\n",
        "for state in states_lower:\n",
        "  # get first half of tweets from GDrive\n",
        "  f = \"drive/My Drive/Thesis/By Location/{}_tweets.csv\".format(state)\n",
        "  df_1 = pd.read_csv(f, lineterminator='\\n')\n",
        "  # get new df, concatenate with old\n",
        "  df_2 = pd.DataFrame(tweets_by_state_2[state], columns=['date', 'id', 'text'])\n",
        "  joint_df = pd.concat([df_1, df_2])\n",
        "  joint_df.to_csv('{}_tweets.csv'.format(state))\n",
        "  !cp $state\"_tweets.csv\" \"drive/My Drive/Thesis/By Location\"\n",
        "\n",
        "# Update counters in GDrive\n",
        "f = \"drive/My Drive/Thesis/By Location/tweet_counters.csv\".format(state)\n",
        "old_counters = pd.read_csv(f, lineterminator='\\n')\n",
        "old_counters = old_counters.set_index('date')\n",
        "new_counters = pd.DataFrame(counters_2, columns=['date', 'success', 'unqueried', 'invalid'])\n",
        "new_counters = new_counters.set_index('date')\n",
        "joint_counters = pd.concat([old_counters, new_counters])\n",
        "joint_counters.to_csv('tweet_counters.csv')\n",
        "!cp \"tweet_counters.csv\" \"drive/My Drive/Thesis/By Location\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy-JR2DNtpQr"
      },
      "source": [
        "### Part Three: 08/08/2020 to 12/31/2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngX4og_ItpQv"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# SET UP FOR ROUND 3 OF LOCATION FINDING: 08/08-12/31\n",
        "# Updated to just use lists and later convert to df for speed\n",
        "# ------------------------------------------------------------------------------\n",
        "tweets_by_state_3 = {} # master df of tweets by state\n",
        "for state in states_lower:tweets_by_state_3[state] = []\n",
        "\n",
        "counters_3 = [] # counters for how many tweets were retained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB6OrXm1tpQv"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# ROUND 3 OF LOCATION FINDING: 08/08-12/31\n",
        "# need to first run cells for find_state_locations_jsonl (in part two) and \n",
        "# check_for_state (in part one)\n",
        "# ------------------------------------------------------------------------------\n",
        "# tweet counts for each\n",
        "daily_counts = [124148, 158325, 117741, 107093, 145354, 66790, 154444, 114484, 103892, 135835, 101112, 95036, 60739, 99370, 117358, 117981, 129937, 98044, 36857, 170240, 159679, 133874, 105210, 84796, 102616, 19631, 162955, 146549, 93257, 89652, 121356, 36922, 117827, 193334, 179803, 152254, 147634, 146124, 110707, 190407, 132233, 52306, 76762, 89633, 99335, 161165, 176233, 148991, 96869, 106853, 87595, 92033, 121104, 148997, 133100, 379026, 436282, 351184, 379817, 421511, 338279, 236816, 142917, 156889, 199151, 158029, 181063, 197712, 151932, 122393, 135731, 145412, 125424, 154964, 149130, 21164, 106903, 163843, 194218, 131788, 185754, 108401, 143916, 154400, 189231, 147902, 159463, 132584, 109027, 172654, 134046, 201783, 137350, 198107, 117073, 87193, 9359, 189160, 103914, 135074, 144441, 233598, 98005, 67618, 163859, 196601, 156792, 126540, 74520, 187570, 154747, 125643, 130822, 142522, 81579, 81744, 159790, 200054, 187236, 160154, 158647, 88053, 16649, 193176, 194171, 176388, 140498, 133113, 132287, 42001, 127344, 214756, 183178, 183070, 225058, 212982, 163172, 150637, 89401, 27306, 108480, 179675, 181734, 223107, 181383]\n",
        "index = 0\n",
        "\n",
        "# Run for all dates in range\n",
        "jsonl_files = glob.glob(\"drive/My Drive/Thesis/1c.CoronaVis jsonl (aug-dec)/*.jsonl\")\n",
        "for jsonl_file in sorted(jsonl_files):\n",
        "  date = jsonl_file[-35:-25]\n",
        "  new_state_dict, success, unqueried, invalid = find_state_locations_jsonl(jsonl_file, date, daily_counts[index])\n",
        "  index += 1\n",
        "  # update state df\n",
        "  for state in states_lower:\n",
        "    tweets_by_state_3[state] = tweets_by_state_3[state] + new_state_dict[state]\n",
        "  # update counters df\n",
        "  total = success + unqueried + invalid\n",
        "  count = {'date': date, 'success': success, 'unqueried': unqueried, 'invalid': invalid, 'total': total}\n",
        "  counters_3.append(count)\n",
        "\n",
        "# Update tweets in GDrive\n",
        "for state in states_lower:\n",
        "  # get first half of tweets from GDrive\n",
        "  f = \"drive/My Drive/Thesis/2. CoronaVis By Location/{}_tweets.csv\".format(state)\n",
        "  df_1 = pd.read_csv(f, lineterminator='\\n')\n",
        "  # get new df, concatenate with old\n",
        "  df_2 = pd.DataFrame(tweets_by_state_3[state], columns=['date', 'id', 'text'])\n",
        "  joint_df = pd.concat([df_1, df_2])\n",
        "  joint_df.to_csv('{}_tweets.csv'.format(state))\n",
        "  !cp $state\"_tweets.csv\" \"drive/My Drive/Thesis/2. CoronaVis By Location\"\n",
        "\n",
        "# Update counters in GDrive\n",
        "f = \"drive/My Drive/Thesis/2. CoronaVis By Location/tweet_counters.csv\"\n",
        "old_counters = pd.read_csv(f, lineterminator='\\n')\n",
        "old_counters = old_counters.set_index('date')\n",
        "new_counters = pd.DataFrame(counters_3, columns=['date', 'success', 'unqueried', 'invalid'])\n",
        "new_counters = new_counters.set_index('date')\n",
        "joint_counters = pd.concat([old_counters, new_counters])\n",
        "joint_counters.to_csv('tweet_counters.csv')\n",
        "!cp \"tweet_counters.csv\" \"drive/My Drive/Thesis/2. CoronaVis By Location\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnAoWyAYvAT1"
      },
      "source": [
        "### Outlier Check: 05/05/2020's low success rate\n",
        "yikes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLffvsPa4nJb"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Load originally hydrated file\n",
        "# ------------------------------------------------------------------------------\n",
        "file = \"drive/My Drive/Thesis/1a.CoronaVis Hydrated/2020-05-05_coronavis_hydrated.csv\"\n",
        "hydrated_05_05 = pd.read_csv(file, lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPhco8wN5EJ1"
      },
      "source": [
        "state_dict, success, unqueried, invalid = find_state_locations(hydrated_05_05, '2020-05-05')\n",
        "print('success: ', success, 'unqueried: ', unqueried, 'invalid: ', invalid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYr_-tcN3gsb"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Load newly hydrated file (note: this was hydrated at a later date than part 1, so \n",
        "# success rate was lower, as more tweets were deleted / made unavailable over time)\n",
        "# ------------------------------------------------------------------------------\n",
        "file = \"drive/My Drive/Thesis/2020-05-05_coronavis_hydrated.jsonl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD0oelnH4Bpt"
      },
      "source": [
        "date = '2020-05-05'\n",
        "new_state_dict, success, unqueried, invalid = find_state_locations_jsonl(file, date, 153462)\n",
        "print('success: ', success, 'unqueried: ', unqueried, 'invalid: ', invalid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoNikWnD5y9F"
      },
      "source": [
        "....dang it lol "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlbbnUdBQ_rQ"
      },
      "source": [
        "### Outlier Check: 11/12/2020's low-ish success rate\n",
        "(this one seems fine)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05cDvtnQ_rZ"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Load newly hydrated file (note: this was hydrated at a later date than part 1, so \n",
        "# success rate was lower, as more tweets were deleted / made unavailable over time)\n",
        "# ------------------------------------------------------------------------------\n",
        "f2_11_12 = \"drive/My Drive/Thesis/2020-11-12_coronavis_hydrated.jsonl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTdTsIdxQ_rZ"
      },
      "source": [
        "date = '2020-10-21'\n",
        "new_state_dict, success, unqueried, invalid = find_state_locations_jsonl(f2_11_12, date, 9282)\n",
        "print('success: ', success, 'unqueried: ', unqueried, 'invalid: ', invalid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f24cUEvhW3c_"
      },
      "source": [
        "joint_counters.loc['2020-11-12']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9h-2T3F_yoX"
      },
      "source": [
        "###Fix 05/05/2020 data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se3sZ8nVCf-2"
      },
      "source": [
        "f = \"drive/My Drive/Thesis/2. CoronaVis By Location/tweet_counters.csv\"\n",
        "old_counters = pd.read_csv(f, lineterminator='\\n')\n",
        "old_counters = old_counters.set_index('date')\n",
        "old_counters.drop('2020-05-05', inplace = True)\n",
        "new_counters = pd.DataFrame(counters_4, columns=['date', 'success', 'unqueried', 'invalid'])\n",
        "new_counters = new_counters.set_index('date')\n",
        "joint_counters = pd.concat([old_counters, new_counters])\n",
        "joint_counters = joint_counters.sort_index()\n",
        "joint_counters.to_csv('tweet_counters.csv')\n",
        "!cp \"tweet_counters.csv\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qai059Zq_1zr"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# ROUND 4 OF LOCATION FINDING: fix 05/05 data\n",
        "# ------------------------------------------------------------------------------\n",
        "# Run state sorting code \n",
        "jsonl_file = file = \"drive/My Drive/Thesis/2020-05-05_coronavis_hydrated.jsonl\"\n",
        "date = '2020-05-05'\n",
        "new_state_dict, success, unqueried, invalid = find_state_locations_jsonl(jsonl_file, date, 153462)\n",
        "# update counters df\n",
        "total = success + unqueried + invalid\n",
        "count = {'date': date, 'success': success, 'unqueried': unqueried, 'invalid': invalid, 'total': total}\n",
        "counters_4 = []\n",
        "counters_4.append(count)\n",
        "\n",
        "# Update tweets in GDrive\n",
        "for state in states_lower:\n",
        "  # get rest of tweets from GDrive\n",
        "  f = \"drive/My Drive/Thesis/2. CoronaVis By Location/{}_tweets.csv\".format(state)\n",
        "  df_1 = pd.read_csv(f, lineterminator='\\n')\n",
        "  # remove previous 05/05 data and unnecessary index columns\n",
        "  df_1.drop('Unnamed: 0', inplace=True, axis=1)\n",
        "  df_1.drop('Unnamed: 0.1', inplace=True, axis=1)\n",
        "  df_1.drop('Unnamed: 0.1.1', inplace=True, axis=1)\n",
        "  df_1.drop(df_1[df_1['date'] == '2020-05-05'].index, inplace = True)\n",
        "  # get new df and concatenate with old\n",
        "  df_2 = pd.DataFrame.from_dict(new_state_dict[state])\n",
        "  joint_df = pd.concat([df_1, df_2])\n",
        "  print(state, \": \", joint_df.shape[0]==(df_1.shape[0] + df_2.shape[0]))\n",
        "  # reorder dates again\n",
        "  joint_df = joint_df.sort_values(by='date')\n",
        "  # save to GDrive\n",
        "  joint_df.to_csv('{}_tweets.csv'.format(state))\n",
        "  !cp $state\"_tweets.csv\" \"drive/My Drive/Thesis/2. CoronaVis By Location\"\n",
        "\n",
        "# Update counters in GDrive\n",
        "f = \"drive/My Drive/Thesis/2. CoronaVis By Location/tweet_counters.csv\"\n",
        "old_counters = pd.read_csv(f, lineterminator='\\n')\n",
        "old_counters = old_counters.set_index('date')\n",
        "old_counters.drop('2020-05-05', inplace = True)\n",
        "new_counters = pd.DataFrame(counters_4, columns=['date', 'success', 'unqueried', 'invalid'])\n",
        "new_counters = new_counters.set_index('date')\n",
        "joint_counters = pd.concat([old_counters, new_counters])\n",
        "joint_counters = joint_counters.sort_index()\n",
        "joint_counters.to_csv('tweet_counters.csv')\n",
        "!cp \"tweet_counters.csv\" \"drive/My Drive/Thesis/2. CoronaVis By Location\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJfvVW3xxI9x"
      },
      "source": [
        "## CoronaVis: Analysis of Location Sorted Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtcuTM_JxS_8"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# COUNT NUMBER OF TWEETS FOR EACH STATE\n",
        "# ------------------------------------------------------------------------------\n",
        "location_files = glob.glob(\"drive/My Drive/Thesis/2. CoronaVis By Location/*.csv\")\n",
        "sorted_tweets = {}\n",
        "num_sorted_tweets = {}\n",
        "for file in sorted(location_files):\n",
        "  if file != \"drive/My Drive/Thesis/2. CoronaVis By Location/tweet_counters.csv\":\n",
        "    state = file[-13:-11]\n",
        "    sorted_tweets[state] = pd.read_csv(file, lineterminator='\\n')\n",
        "    num_sorted_tweets[state] = len(sorted_tweets[state])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jee7DA3UJcqB"
      },
      "source": [
        "num_sorted_tweets_df = pd.DataFrame(list(num_sorted_tweets.items()),columns = ['state', 'count']) \n",
        "# num_sorted_tweets_df = num_sorted_tweets_df.set_index('state')\n",
        "num_sorted_tweets_df['state'] = num_sorted_tweets_df.apply(lambda row: row.state.upper(), axis=1)\n",
        "num_sorted_tweets_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsYEiRKDXy_P"
      },
      "source": [
        "total = 0\n",
        "for state in num_sorted_tweets:\n",
        "  total += num_sorted_tweets[state]\n",
        "print(\"Total number of successfully sorted tweets:\", total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx-0lh7tT09b"
      },
      "source": [
        "counters_file = \"drive/My Drive/Thesis/2. CoronaVis By Location/tweet_counters.csv\"\n",
        "joint_counters = pd.read_csv(counters_file, lineterminator='\\n')\n",
        "joint_counters = joint_counters.set_index('date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqCT1xGbaubI"
      },
      "source": [
        "joint_counters['total'].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxJP4gUAfDRW"
      },
      "source": [
        "# Title: Total Number of Tweets Available Per State\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.catplot(\n",
        "    data=num_sorted_tweets_df, kind=\"bar\",\n",
        "    x=\"state\", y=\"count\", \n",
        "    height=15, aspect=2.5, palette=\"Set2\", edgecolor=\".6\"\n",
        ").set_axis_labels(\"State\", \"Number of Tweets\", labelpad=35)\n",
        "g.set_xticklabels(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzZLT23lzW3s"
      },
      "source": [
        "# visualize number of tweets successfully hydrated\n",
        "# no longer used (see seaborn one above)\n",
        "x,y = zip(*num_sorted_tweets.items())\n",
        "fig, ax = plt.subplots(figsize=(35,5))\n",
        "ax.set_title('Number of Tweets Available per State')\n",
        "plt.xticks(rotation=90)\n",
        "plt.bar(x, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB1Hoz7MztVW"
      },
      "source": [
        "# todo: rewrite this code to actually call min and max functions \n",
        "print('max is CA with {} tweets'.format(num_sorted_tweets['ca']))\n",
        "print('min is WY with {} tweets'.format(num_sorted_tweets['wy']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJvUzN3VBwk4"
      },
      "source": [
        "# calculate percentages and save into the gdrive\n",
        "# already done!\n",
        "joint_counters['total'] = joint_counters.success + joint_counters.invalid + joint_counters.unqueried\n",
        "joint_counters['%success'] = joint_counters.apply(lambda row: row.success / row.total, axis=1)\n",
        "joint_counters['%invalid'] = joint_counters.apply(lambda row: row.invalid / row.total, axis=1)\n",
        "joint_counters['%unqueried'] = joint_counters.apply(lambda row: row.unqueried / row.total, axis=1)\n",
        "\n",
        "joint_counters.to_csv('tweet_counters.csv')\n",
        "# !cp \"tweet_counters.csv\" \"drive/My Drive/Thesis/2. CoronaVis By Location\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2dqUPNgEGz7"
      },
      "source": [
        "print('mean of success rate: ', joint_counters['%success'].mean())\n",
        "print('mean of unqueried rate: ', joint_counters['%unqueried'].mean())\n",
        "print('mean of invalid rate: ', joint_counters['%invalid'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYf55HdGAPo_"
      },
      "source": [
        "# create a reformated df for the purpose of a multi-line graph\n",
        "date = joint_counters.index.tolist()\n",
        "length = len(date)\n",
        "date = date + date + date\n",
        "percent = joint_counters['%success'].tolist() + joint_counters['%invalid'].tolist() + joint_counters['%unqueried'].tolist()\n",
        "result = ['success'] * length + ['not_in_USA'] * length + ['undetermined'] * length\n",
        "\n",
        "state_classification = pd.DataFrame({'date': date, 'percent': percent, 'State Result': result})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiY4ey1opVqK"
      },
      "source": [
        "# find where to place ticks (start of each month) and set their labels\n",
        "month_ticks = ['Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "tick_indices = []\n",
        "\n",
        "curr_month = ''\n",
        "for i, date in enumerate(joint_counters.index.tolist()):\n",
        "  month = date[5:7]\n",
        "  if month != curr_month:\n",
        "    tick_indices.append(i)\n",
        "    curr_month = month"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AeCCNz68aQC"
      },
      "source": [
        "# Title: Tweet State Attribution Results: An Overview\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=state_classification, kind=\"line\", \n",
        "    x='date', y='percent', hue='State Result',\n",
        "    height=15, aspect=2.5, palette=\"Set2\"\n",
        ").set_axis_labels(\"Date (Month in 2020)\", \"Percent (%)\", labelpad=35)\n",
        "g.set(xticks=tick_indices)\n",
        "g.set_xticklabels(month_ticks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPKlQYUCqApQ"
      },
      "source": [
        "# Title: Tweet State Attribution Results: Percent Successful\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=joint_counters, kind=\"line\", \n",
        "    x=joint_counters.index, y='%success', \n",
        "    height=15, aspect=2.5, palette=\"Set2\"\n",
        ").set_axis_labels(\"Date (Month in 2020)\", \"Percent (%)\", labelpad=35)\n",
        "g.set(xticks=tick_indices)\n",
        "g.set_xticklabels(month_ticks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWbu-TOvsHqS"
      },
      "source": [
        "# Title: Tweet State Attribution Results: Percent With Non-USA Country Code\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=joint_counters, kind=\"line\", \n",
        "    x=joint_counters.index, y='%invalid', \n",
        "    height=15, aspect=2.5, palette=\"Set2\"\n",
        ").set_axis_labels(\"Date (Month in 2020)\", \"Percent (%)\", labelpad=35)\n",
        "g.set(xticks=tick_indices)\n",
        "g.set_xticklabels(month_ticks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFlluq6SsXWZ"
      },
      "source": [
        "# Title: Tweet State Attribution Results: Percent Undetermined\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=joint_counters, kind=\"line\", \n",
        "    x=joint_counters.index, y='%unqueried', \n",
        "    height=15, aspect=2.5, palette=\"Set2\"\n",
        ").set_axis_labels(\"Date (Month in 2020)\", \"Percent (%)\", labelpad=35)\n",
        "g.set(xticks=tick_indices)\n",
        "g.set_xticklabels(month_ticks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxtjYxQQ0zuC"
      },
      "source": [
        "# ignore this, this is an awful graphic LOL\n",
        "joint_counters.plot.barh(y=['%success', '%invalid', '%unqueried'], stacked=True, color={\"%invalid\": \"red\", \"%success\": \"LightGreen\", \"%unqueried\": \"LightBlue\"}, figsize=(20,75));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNgqC2eT25U"
      },
      "source": [
        "##3. Sentiment Analysis of Tweets by Location\n",
        "Performed using VADER: https://github.com/cjhutto/vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p0j6lK7lYZw"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# SET UP VADER, INITIALIZE ANALYZER\n",
        "# ------------------------------------------------------------------------------\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyser = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOe4ByIO1UsT"
      },
      "source": [
        "print(states_lower)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRiYnkB-HaCB"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# HANDLE STATES IN SMALL GROUPS TO AVOID SESSION CRASH AFTER USING ALL AVAILABLE RAM\n",
        "# ------------------------------------------------------------------------------\n",
        "set1 = ['al', 'ak', 'az', 'ar', 'co', 'ca', 'ct', 'de', 'ga', 'hi', 'id'] # done\n",
        "set2 = ['fl', 'il', 'in', 'ia', 'ks', 'ky'] # done\n",
        "set3 = ['la', 'me', 'md', 'ma', 'mi', 'mn', 'ms', 'mo', 'mt', 'ne', 'nv', 'nh'] # done\n",
        "set4 = ['nj', 'nm', 'ny', 'nc', 'nd', 'oh', 'ok', 'or', 'pa', 'ri', 'sc', 'sd', 'tn'] # done\n",
        "set5 = ['tx', 'ut', 'vt', 'va', 'wa', 'wv', 'wi', 'wy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JtJAz0KUGgs"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# GET HYDRATED, SORTED TWEET CSVs FROM GDRIVE\n",
        "# hydrated_dict: {yyyy-mm-dd: tweets}\n",
        "# ------------------------------------------------------------------------------\n",
        "location_files = glob.glob(\"drive/My Drive/Thesis/2. CoronaVis By Location/*.csv\")\n",
        "tweets = {}\n",
        "for file in sorted(location_files):\n",
        "  state = file[-13:-11]\n",
        "  if state in set1: # CHANGE SET HERE\n",
        "    tweets[state] = pd.read_csv(file, lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep38hxz-m2kh"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# DEFINE FUNCTIONS TO CLEAN THROUGH TWEETS\n",
        "# CODE ADAPTED FROM : https://medium.com/python-in-plain-english/twitter-sentiment-analysis-using-vader-tweepy-b2a62fba151e\n",
        "# ------------------------------------------------------------------------------\n",
        "def remove_pattern(input_txt, pattern1, pattern2, pattern3):\n",
        "  r1 = re.findall(pattern1, input_txt)\n",
        "  for i in r1:\n",
        "    input_txt = re.sub(i, '', input_txt)\n",
        "\n",
        "  r2 = re.findall(pattern2, input_txt)\n",
        "  for i in r2:\n",
        "    input_txt = re.sub(i, '', input_txt)\n",
        "\n",
        "  r3 = re.findall(pattern3, input_txt)\n",
        "  for i in r3:\n",
        "    input_txt = re.sub(i, '', input_txt)  \n",
        "                \n",
        "  return input_txt\n",
        "\n",
        "def clean_tweets(tweets):\n",
        "  #remove twitter Return handles (RT @xxx:), twitter handles (@xxx), URL links (httpxxx)\n",
        "  tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\", \"@[\\w]*\", \"https?://[A-Za-z0-9./]*\")\n",
        "  return tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zyQu-RhoDWd"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# CLEAN THROUGH TWEETS\n",
        "# ------------------------------------------------------------------------------\n",
        "index = 1\n",
        "for state in sorted(tweets.keys()):\n",
        "  print(index, state, datetime.now().time().strftime('%H:%M:%S'))\n",
        "  tweets[state]['clean_text'] = clean_tweets(tweets[state]['text'])\n",
        "  index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JEaaEmpX67p"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# REMOVE UNNECESSARY COLS FROM DFs\n",
        "# ------------------------------------------------------------------------------\n",
        "for state in tweets.keys():\n",
        "  tweets[state].drop('Unnamed: 0', inplace=True, axis=1)\n",
        "  tweets[state].drop('Unnamed: 0.1', inplace=True, axis=1)\n",
        "  tweets[state].drop('Unnamed: 0.1.1', inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvzLYrDZpzb4"
      },
      "source": [
        "for state in sorted(tweets.keys()):\n",
        "  scores_list = []\n",
        "  for index, row in tqdm(tweets[state].iterrows(), total=tweets[state].shape[0], desc=state):\n",
        "    text = row['clean_text']\n",
        "    sentiment = analyser.polarity_scores(text)\n",
        "    com = sentiment[\"compound\"]\n",
        "    pos = sentiment[\"pos\"]\n",
        "    neu = sentiment[\"neu\"]\n",
        "    neg = sentiment[\"neg\"]\n",
        "      \n",
        "    scores_list.append({\"Compound\": com, \"Positive\": pos, \"Negative\": neg, \"Neutral\": neu})\n",
        "\n",
        "  sentiment_scores = pd.DataFrame.from_dict(scores_list)\n",
        "  tweets[state] = tweets[state].join(sentiment_scores)\n",
        "\n",
        "  tweets[state].to_csv('{}_tweets_sentiment.csv'.format(state))\n",
        "  !cp $state\"_tweets_sentiment.csv\" \"drive/My Drive/Thesis/3. CoronaVis Sentiment\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCNh3mXa-lwu"
      },
      "source": [
        "for state in sorted(tweets.keys()):\n",
        "  print(state, list(tweets[state].columns.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LtWEZWmfAbb"
      },
      "source": [
        "### Fix 05/05 Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUHvO3f6fCWp"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Fix sentiment analysis of just 05/05 data\n",
        "# ------------------------------------------------------------------------------\n",
        "for state in states_lower:\n",
        "  # Perform sentiment analysis on 05/05/2020 data for the state\n",
        "  new_df = pd.DataFrame.from_dict(new_state_dict[state])\n",
        "  new_df['clean_text'] = clean_tweets(new_df['text'])\n",
        "\n",
        "  scores_list = []\n",
        "  for index, row in tqdm(new_df.iterrows(), total=new_df.shape[0], desc=state):\n",
        "    text = row['clean_text']\n",
        "    sentiment = analyser.polarity_scores(text)\n",
        "    com = sentiment[\"compound\"]\n",
        "    pos = sentiment[\"pos\"]\n",
        "    neu = sentiment[\"neu\"]\n",
        "    neg = sentiment[\"neg\"]\n",
        "      \n",
        "    scores_list.append({\"Compound\": com, \"Positive\": pos, \"Negative\": neg, \"Neutral\": neu})\n",
        "\n",
        "  sentiment_scores = pd.DataFrame.from_dict(scores_list)\n",
        "  new_df = new_df.join(sentiment_scores)\n",
        "\n",
        "  # get original sentiment analyzed state file\n",
        "  f = \"drive/My Drive/Thesis/3. CoronaVis Sentiment/{}_tweets_sentiment.csv\".format(state)\n",
        "  og_df = pd.read_csv(f, lineterminator='\\n')\n",
        "  # remove previous 05/05 data and unnecessary index columns\n",
        "  og_df.drop('Unnamed: 0', inplace=True, axis=1)\n",
        "  og_df.drop(og_df[og_df['date'] == '2020-05-05'].index, inplace = True)\n",
        "  # get new df and concatenate with old\n",
        "  joint_df = pd.concat([og_df, new_df])\n",
        "  # reorder dates again\n",
        "  joint_df = joint_df.sort_values(by='date')\n",
        "  print(joint_df.shape[0])\n",
        "  # # save to GDrive\n",
        "  joint_df.to_csv('{}_tweets_sentiment.csv'.format(state))\n",
        "  !cp $state\"_tweets_sentiment.csv\" \"drive/My Drive/Thesis/3. CoronaVis Sentiment\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcC_AYiqMxz-"
      },
      "source": [
        "# **Prepare Input/Output Dataframes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP9c2rO3U7b4"
      },
      "source": [
        "###Load COVID Historical Data from CSV files in GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P1Ke6YCpx2N"
      },
      "source": [
        "# function to turn date string '2020-03-05' into int 20200305\n",
        "def dateStrToInt(row):\n",
        "  date = row['date']\n",
        "  return np.int(date[:4] + date[5:7] + date[8:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mhQjsWlU-LS"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# load state COVID case data from GDrive, save into master_history: {state: df}\n",
        "# data loaded in 2 parts\n",
        "# ------------------------------------------------------------------------------\n",
        "master_history_1 = {} # empty dict to fill with dfs\n",
        "for state in states_lower:\n",
        "  date = '2020-10-27' # date you want to load historical data from\n",
        "  path = 'drive/My Drive/Thesis/{}/{}_{}_historical.csv'.format(date, date, state)\n",
        "  master_history_1[state] = pd.read_csv(path, index_col=0).iloc[::-1].reset_index().drop(columns=['index'])\n",
        "\n",
        "master_history_2 = {} # empty dict to fill with dfs\n",
        "for state in states_lower:\n",
        "  date = '2021-03-12' # date you want to load historical data from\n",
        "  path = 'drive/My Drive/Thesis/{}/{}_{}_historical.csv'.format(date, date, state)\n",
        "  master_history_2[state] = pd.read_csv(path, index_col=0).iloc[::-1].reset_index().drop(columns=['index'])\n",
        "\n",
        "master_history_3 = {} # empty dict to fill with dfs\n",
        "for state in states_lower:\n",
        "  path = 'drive/My Drive/Thesis/dataQualityGrades/{}_dataQualityGrade.csv'.format(state)\n",
        "  master_history_3[state] = pd.read_csv(path).iloc[::-1].reset_index().drop(columns=['index', 'Unnamed: 0'])\n",
        "  master_history_3[state]['date'] = master_history_3[state].apply(dateStrToInt, axis=1)\n",
        "  master_history_3[state] = master_history_3[state].set_index('date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7e48I2dwlis"
      },
      "source": [
        "# double check final date in all master_history_1 dfs is 1026 -- it is :)\n",
        "# for state in states_lower:\n",
        "  # print(master_history_1[state].tail(1)['date'] == 20201026)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wn-FY39Vcn4"
      },
      "source": [
        "# # ------------------------------------------------------------------------------\n",
        "# # data exploration\n",
        "# # ------------------------------------------------------------------------------\n",
        "# master_history['va'].loc[:368]['positive'].isnull().values.any()\n",
        "# # master_history['nj']\n",
        "# master_history['va'].loc[365:370]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzL-d6J-Kxlq"
      },
      "source": [
        "**Note**: positiveIncrease category has no NaN values. The 'positive' category either has 0 values or NaN values until the first report of cases.\n",
        "\n",
        "**Categories to use**: positive, positiveIncrease, negative, dataQualityGrade (quantified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNT1mD6NPTgJ"
      },
      "source": [
        "### COVID Historical Data: prep for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E49P1OnBikbk"
      },
      "source": [
        "def replaceDataQualityGrade(row):\n",
        "  if row['dataQualityGrade'] is None:\n",
        "    return master_history_3[state].loc[row['date']]['dataQualityGrade']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xfxG4hawYIf"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# combine two sets of data\n",
        "# ------------------------------------------------------------------------------\n",
        "master_history = {}\n",
        "for state in states_lower:\n",
        "  master_history_2[state].drop(master_history_2[state][master_history_2[state].date < 20201027].index, inplace=True)\n",
        "  master_history_2[state].drop(master_history_2[state][master_history_2[state].date > 20201231].index, inplace=True)\n",
        "  master_history[state] = pd.concat([master_history_1[state], master_history_2[state]])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62qvFAvOnIUY"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# fill in dataQualityGrade data from new API (v2)\n",
        "# ------------------------------------------------------------------------------\n",
        "for state in states_lower:\n",
        "  master_history[state]['dataQualityGrade'] = master_history[state].apply(replaceDataQualityGrade, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9xPIrGOwXZy"
      },
      "source": [
        "master_history['tx'].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6H3gIYf2VS9"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# get list of states which have NaN values in the positive category\n",
        "# ------------------------------------------------------------------------------\n",
        "nanStates = []\n",
        "for state in states_lower:\n",
        "  if master_history[state]['positive'].isnull().values.any():\n",
        "    nanStates.append(state)\n",
        "\n",
        "nanStates # print list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8OcM8Ew2frs"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# remove all rows with NaN / zero values in the positive column so it starts \n",
        "# with first positive cases\n",
        "# (this gets rid of NaN values in the positiveIncrease column too)\n",
        "# ------------------------------------------------------------------------------\n",
        "for state in states_lower:\n",
        "  firstIndex = -1\n",
        "  if state in nanStates:\n",
        "    firstIndex = master_history[state]['positive'].first_valid_index()\n",
        "  elif master_history[state]['positive'].values[-1] == 0:\n",
        "    firstIndex = master_history[state].positive.eq(0).idxmax()\n",
        "\n",
        "  if firstIndex != -1:\n",
        "    master_history[state] = master_history[state].iloc[firstIndex:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb4zcBpmGEwD"
      },
      "source": [
        "# double check\n",
        "for state in states_lower:\n",
        "  if master_history[state]['positive'].isnull().values.any():\n",
        "    print(state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx8ukfcgzFyf"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# zero out all null values in the negative column\n",
        "# ------------------------------------------------------------------------------\n",
        "for state in states_lower:\n",
        "  if master_history[state]['negative'].isnull().values.any():\n",
        "    # print(state)\n",
        "    master_history[state]['negative'] = master_history[state]['negative'].fillna(0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# previously: check where the NaN values in the negative column are and zero \n",
        "# them out if they're all at the end\n",
        "# ------------------------------------------------------------------------------\n",
        "  # if master_history[state]['negative'].isnull().values.any():\n",
        "    # nullList = np.where(master_history[state]['negative'].isnull())[0]\n",
        "    # rangeList=np.arange(nullList[0], master_history[state].shape[0])\n",
        "    # if (rangeList == nullList).all():\n",
        "    #   master_history[state]['negative'] = master_history[state]['negative'].fillna(0)\n",
        "    #   print(state, 'nulls at the end, zeroed out! :)')\n",
        "    # else: \n",
        "    #   print(state, ': nulls, not all at end, need to fix :(')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL_woBsxBx2F"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# replace dataQualityGrade with a numerical score (assign NaN equivalent to a C)\n",
        "# ------------------------------------------------------------------------------\n",
        "def numberGrade(row):\n",
        "  if row['dataQualityGrade'] == 'A+':\n",
        "    return 100\n",
        "  elif row['dataQualityGrade'] == 'A':\n",
        "    return 95\n",
        "  elif row['dataQualityGrade'] == 'B':\n",
        "    return 85\n",
        "  elif row['dataQualityGrade'] == 'C':\n",
        "    return 75\n",
        "  elif row['dataQualityGrade'] == 'D':\n",
        "    return 65\n",
        "  elif row['dataQualityGrade'] == 'F':\n",
        "    return 55\n",
        "  else:\n",
        "    return 75 # what score to give when they don't have a score??\n",
        "\n",
        "for state in states_lower:\n",
        "  master_history[state]['dataQualityGrade'] = master_history[state].apply(numberGrade, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8egSZSXzbZ8"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Continue cleaning up df: reformat date, flip order of rows to be consistent \n",
        "# with the twitter df, rename columns, remove hositalizedIncrease and \n",
        "# deathIncrease columns for poor data quality\n",
        "# ------------------------------------------------------------------------------\n",
        "def dateFormat(date):\n",
        "  date = str(date)\n",
        "  return date[0:4] + \"-\" + date[4:6] + \"-\" + date[6:8]\n",
        "\n",
        "for state in states_lower:\n",
        "  master_history[state] = master_history[state][::-1].reset_index().drop(columns=['index', 'hospitalizedIncrease', 'deathIncrease'])\n",
        "  master_history[state]['date'] = [dateFormat(date) for date in master_history[state]['date']]\n",
        "  master_history[state].columns = ['date', 'posCases', 'posIncCases', 'negCases', 'dataQualityGrade']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgJqHy7wHd0K"
      },
      "source": [
        "# example state\n",
        "master_history['wa']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZl5puPBN3us"
      },
      "source": [
        "###Tweet Data: Prepare for model (only run once; saved to GDrive after)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7XP1yP-NKUP"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# GET MOST RECENT TWEET CSVs FROM GDRIVE\n",
        "# master_tweets: {state: tweets}\n",
        "# ------------------------------------------------------------------------------\n",
        "location_files = glob.glob(\"drive/My Drive/Thesis/3. CoronaVis Sentiment/*.csv\")\n",
        "master_tweets = {}\n",
        "for file in sorted(location_files):\n",
        "  state = file[-23:-21]\n",
        "  master_tweets[state] = pd.read_csv(file, lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvA2Sw-eIxct"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# for each state, find daily tweet counts and sentiment averages across tweets\n",
        "# save to Google Drive (folder: 4. Daily Tweet Summaries)\n",
        "# ------------------------------------------------------------------------------\n",
        "for state in states_lower:\n",
        "  daily_tweets = {}\n",
        "  date = '2020-03-05' # first date for the tweets\n",
        "  scores = {'count': 0, 'Compound': 0, 'Positive': 0, 'Negative': 0, 'Neutral': 0}\n",
        "  for index, row in tqdm(master_tweets[state].iterrows(), total=master_tweets[state].shape[0], desc=state):\n",
        "    if row['date'] == date:\n",
        "      scores['count'] += 1\n",
        "      scores['Compound'] += row['Compound']\n",
        "      scores['Positive'] += row['Positive']\n",
        "      scores['Negative'] += row['Negative']\n",
        "      scores['Neutral'] += row['Neutral']\n",
        "    else:\n",
        "      # find average scores add prev date to the dict\n",
        "      scores['Compound'] = scores['Compound'] / scores['count']\n",
        "      scores['Positive'] = scores['Positive'] / scores['count']\n",
        "      scores['Negative'] = scores['Negative'] / scores['count']\n",
        "      scores['Neutral'] = scores['Neutral'] / scores['count']\n",
        "      daily_tweets[date] = scores\n",
        "      # restart dict and update date\n",
        "      date = row['date']\n",
        "      scores = {'count': 1, 'Compound': row['Compound'], 'Positive': row['Positive'], \n",
        "                'Negative': row['Negative'], 'Neutral': row['Neutral']}\n",
        "  \n",
        "  # add final date to dict if it hasn't been added yet\n",
        "  if date not in daily_tweets:\n",
        "    scores['Compound'] = scores['Compound'] / scores['count']\n",
        "    scores['Positive'] = scores['Positive'] / scores['count']\n",
        "    scores['Negative'] = scores['Negative'] / scores['count']\n",
        "    scores['Neutral'] = scores['Neutral'] / scores['count']\n",
        "    daily_tweets[date] = scores\n",
        "\n",
        "  # save to GDrive\n",
        "  daily_tweets_df = pd.DataFrame.from_dict(daily_tweets, orient='index')\n",
        "  daily_tweets_df.to_csv('{}_daily_tweet_summaries.csv'.format(state))\n",
        "  !cp $state\"_daily_tweet_summaries.csv\" \"drive/My Drive/Thesis/4. Daily Tweet Summaries\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyds0__agbbN"
      },
      "source": [
        "###Tweet Data: Fix 05/05/2020 Data (only run once; saved to GDrive after)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB5xQHJ4gbbT"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Update tweet summaries with new 05/05 data\n",
        "# ------------------------------------------------------------------------------\n",
        "states = ['az', 'ar', 'ca', 'co', 'ct', 'de', 'fl', 'ga', 'hi', 'id', 'il', 'in', 'ia', 'ks', 'ky', 'la', 'me', 'md', 'ma', 'mi', 'mn', 'ms', 'mo', 'mt', 'ne', 'nv', 'nh', 'nj', 'nm', 'ny', 'nc', 'nd', 'oh', 'ok', 'or', 'pa', 'ri', 'sc', 'sd', 'tn', 'tx', 'ut', 'vt', 'va', 'wa', 'wv', 'wi', 'wy']\n",
        "\n",
        "for state in states:\n",
        "  # get full tweet sentiment CSV\n",
        "  f_sentiment = \"drive/My Drive/Thesis/3. CoronaVis Sentiment/{}_tweets_sentiment.csv\".format(state)\n",
        "  sentiment = pd.read_csv(f_sentiment, lineterminator='\\n')\n",
        "\n",
        "  # get current tweet summaries sheet and remove 05/05 data\n",
        "  f_summary = \"drive/My Drive/Thesis/4. Daily Tweet Summaries/{}_daily_tweet_summaries.csv\".format(state)\n",
        "  summary = pd.read_csv(f_summary, lineterminator='\\n')\n",
        "  summary.rename(columns = {'Unnamed: 0':'date'}, inplace = True)\n",
        "  summary.drop(summary[summary['date'] == '2020-05-05'].index, inplace = True)\n",
        "\n",
        "  sentiment_05_05 = sentiment.loc[sentiment['date'] == '2020-05-05']\n",
        "  scores = {'count': 0, 'Compound': 0, 'Positive': 0, 'Negative': 0, 'Neutral': 0}\n",
        "  date = 'temp'\n",
        "  for index, row in tqdm(sentiment_05_05.iterrows(), total=sentiment_05_05.shape[0], desc=state):\n",
        "    if row['date'] == date:\n",
        "      scores['count'] += 1\n",
        "      scores['Compound'] += row['Compound']\n",
        "      scores['Positive'] += row['Positive']\n",
        "      scores['Negative'] += row['Negative']\n",
        "      scores['Neutral'] += row['Neutral']\n",
        "    else:\n",
        "      # restart dict and update date\n",
        "      date = row['date']\n",
        "      scores = {'count': 1, 'Compound': row['Compound'], 'Positive': row['Positive'], \n",
        "                'Negative': row['Negative'], 'Neutral': row['Neutral']}\n",
        "  \n",
        "  scores['Compound'] = scores['Compound'] / scores['count']\n",
        "  scores['Positive'] = scores['Positive'] / scores['count']\n",
        "  scores['Negative'] = scores['Negative'] / scores['count']\n",
        "  scores['Neutral'] = scores['Neutral'] / scores['count']\n",
        "  scores['date'] = '2020-05-05'\n",
        "\n",
        "  # get new df and concatenate with old\n",
        "  new_data = pd.DataFrame([scores])\n",
        "  joint_df = pd.concat([summary, new_data])\n",
        "  # reorder dates again\n",
        "  joint_df = joint_df.sort_values(by='date')\n",
        "\n",
        "  # save to GDrive\n",
        "  joint_df.to_csv('{}_daily_tweet_summaries.csv'.format(state))\n",
        "  !cp $state\"_daily_tweet_summaries.csv\" \"drive/My Drive/Thesis/4. Daily Tweet Summaries\"\n",
        "  print('done with ', state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dskyp2RsuJyo"
      },
      "source": [
        "###Combine the dfs into one large prepared input/output df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY3WD3gwuRNj"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Grab daily tweet info from GDrive (assumes the COVID data is already loaded\n",
        "# on Colab because it runs quickly)\n",
        "# ------------------------------------------------------------------------------\n",
        "location_files = glob.glob(\"drive/My Drive/Thesis/4. Daily Tweet Summaries/*.csv\")\n",
        "daily_tweets = {}\n",
        "for file in sorted(location_files):\n",
        "  state = file[-28:-26]\n",
        "  daily_tweets[state] = pd.read_csv(file, lineterminator='\\n')\n",
        "  daily_tweets[state].drop('Unnamed: 0', inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRMfBgaxDwMb"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# rename columns in df daily_tweets\n",
        "# ------------------------------------------------------------------------------\n",
        "for state in states_lower:\n",
        "  daily_tweets[state].columns = ['date', 'tweetCount', 'compoundSent', 'posSent', 'negSent', 'neutralSent']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Uve_39j46Gu"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# fix order of dates\n",
        "# ------------------------------------------------------------------------------\n",
        "for state in states_lower:\n",
        "  daily_tweets[state] = daily_tweets[state].sort_values(by = 'date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEvgk2jSGuDD"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# def state populations and densities and a function to access them\n",
        "# source: https://worldpopulationreview.com/states\n",
        "# ------------------------------------------------------------------------------\n",
        "state_pops = [4934190, 724357, 7520100, 3033950, 39613500, 5893630, 3552820, 990334, 21944600, 10830000, 1406430, 1860120, 12569300, 6805660, 3167970, 2917220, 4480710, 4627000, 1354520, 6065440, 6912240, 9992430, 5706400, 2966410, 6169040, 1085000, 1952000, 3185790, 1372200, 8874520, 2105000, 19300000, 10701000, 770026, 11714600, 3990440, 4289440, 12804100, 1061510, 5277830, 896581, 6944260, 29730300, 3310770, 623251, 8603980, 7796940, 1767860, 5852490, 581075]\n",
        "state_densities = [97.4270, 1.2694, 66.2016, 58.3059, 254.2929, 56.8653, 733.7505, 508.1242, 409.2233, 188.3053, 218.9678, 22.5079, 226.3964, 189.9643, 56.7157, 35.6807, 113.4759, 107.0966, 43.9166, 624.8522, 886.1846, 176.7352, 71.6641, 63.2187, 89.7419, 7.4547, 25.4087, 29.0195, 153.2671, 1206.7609, 17.3540, 409.5404, 220.1037, 11.1596, 286.6939, 58.1739, 44.6873, 286.1699, 1026.6054, 175.5707, 11.8265, 168.4069, 113.8080, 40.2917, 67.6197, 217.8774, 117.3248, 73.5444, 108.0633, 5.9847]\n",
        "\n",
        "def getStateInfo(state):\n",
        "  index = states_lower.index(state)\n",
        "  return state_pops[index], state_densities[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vogO8c10NEnR"
      },
      "source": [
        "def getColNames(label):\n",
        "  return [label + '_tweetCount', label + '_CompoundSent', label + '_posSent', \n",
        "          label + '_negSent', label + '_neutralSent', label + '_posCases',\n",
        "          label + '_posIncCases', label + '_negCases', label + '_dataQualityGrade']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxye5ezc015_"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Create the master input/output tables by state {state: I/O df}\n",
        "# ------------------------------------------------------------------------------\n",
        "io_dict = {}\n",
        "for state in tqdm(states_lower, total=50):\n",
        "  # merge tables for all rows with dates in common\n",
        "  daily_df = pd.merge(daily_tweets[state], master_history[state], on=\"date\")\n",
        "\n",
        "  # calculate rolling averages\n",
        "  avg3_means = daily_df.rolling(3).mean() # 3 day rolling average\n",
        "  avg3_means.columns = getColNames('mean3')\n",
        "  avg3_medians = daily_df.rolling(3).median() # 3 day rolling median\n",
        "  avg3_medians.columns = getColNames('median3')\n",
        "  avg7_means = daily_df.rolling(7).mean() # 7 day rolling average\n",
        "  avg7_means.columns = getColNames('mean7')\n",
        "  avg7_medians = daily_df.rolling(7).median() # 7 day rolling median\n",
        "  avg7_medians.columns = getColNames('median7')\n",
        "\n",
        "  # merge rolling average tables\n",
        "  daily_df.columns = ['date'] + getColNames('today')\n",
        "  frames = [daily_df, avg3_means, avg3_medians, avg7_means, avg7_medians]\n",
        "  rolling_df = pd.concat(frames, axis=1)\n",
        "\n",
        "  # add state population and density to every row\n",
        "  pop, density = getStateInfo(state)\n",
        "  rolling_df['population'] = pop\n",
        "  rolling_df['density'] = density\n",
        "\n",
        "  # add output columns: shift to stay with the date it refers to\n",
        "  # col was renamed last for earlier but data still represents that day\n",
        "  predict3 = rolling_df['today_posCases'].shift(-3)\n",
        "  predict7 = rolling_df['today_posCases'].shift(-7)\n",
        "  predict14 = rolling_df['today_posCases'].shift(-14)\n",
        "  io_frames = [rolling_df, predict3, predict7, predict14]\n",
        "  state_df = pd.concat(io_frames, axis=1)\n",
        "  state_df.columns = list(rolling_df.columns) + ['predict3', 'predict7', 'predict14']\n",
        "\n",
        "  # remove the first 6 rows (don't have 7-day rolling average) and last 14 rows \n",
        "  # (don't have predictions 3, 7, and 14 days ahead)\n",
        "  state_df = state_df[6:-14]\n",
        "\n",
        "  # save to i/o dict\n",
        "  io_dict[state] = state_df\n",
        "\n",
        "  # # save to GDrive\n",
        "  state_df.to_csv('{}_IO.csv'.format(state))\n",
        "  !cp $state\"_IO.csv\" \"drive/My Drive/Thesis/5. IO Table\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II09l7MbflE-"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Create a master input/output df with all state data\n",
        "# ------------------------------------------------------------------------------\n",
        "df_list = []\n",
        "for state in states_lower:\n",
        "  df_list.append(io_dict[state])\n",
        "\n",
        "# concatenate all the state dfs into a master df\n",
        "master_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# save master df to GDrive\n",
        "master_df.to_csv('master_IO.csv'.format(state))\n",
        "!cp \"master_IO.csv\" \"drive/My Drive/Thesis/5. IO Table\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOe1-N82w011"
      },
      "source": [
        "#**Some Additional Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Plw3V3-w4vg"
      },
      "source": [
        "##COVID Tracking Project Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ogtx-g1N7r"
      },
      "source": [
        "###Set up: DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YKGGNsjq6x-"
      },
      "source": [
        "# define regions of the US\n",
        "# census bureau designated\n",
        "div1 = ['ct', 'me', 'ma', 'nh', 'ri', 'vt'] # northeast: new england\n",
        "# div1 = ['nh', 'ma', 'ct', 'ri', 'vt', 'me']\n",
        "div2 = ['nj', 'ny', 'pa'] # northeast: mid-atlantic\n",
        "div3 = ['il', 'in', 'mi', 'oh', 'wi'] # midwest: east north central\n",
        "div4 = ['ia', 'ks', 'mn', 'mo', 'ne', 'nd', 'sd'] #midwest: west north central\n",
        "div5 = ['de', 'fl', 'ga', 'md', 'nc', 'sc', 'va', 'wv'] # south atlantic\n",
        "div6 = ['al', 'ky', 'ms', 'tn'] # south: east south central\n",
        "div7 = ['ar', 'la', 'ok', 'tx'] # south: west south central\n",
        "div8 = ['az', 'co', 'id', 'mt', 'nv', 'nm', 'ut', 'wy'] # west: mountain\n",
        "div9 = ['ak', 'ca', 'hi', 'or', 'wa'] # west: pacific\n",
        "\n",
        "regions = [div1, div2, div3, div4, div5, div6, div7, div8, div9]\n",
        "region_names = ['New England', 'Mid-Atlantic', 'East North Central', \n",
        "                'West North Central', 'South Atlantic', 'East South Central',\n",
        "                'West South Central', 'Mountain', 'Pacific']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxSdCkdh1QfF"
      },
      "source": [
        "# compile master_history df into one large df for purpose of graph creation\n",
        "history = []\n",
        "\n",
        "for i, div in enumerate(regions):\n",
        "  region_df = pd.DataFrame()\n",
        "  for state in div:\n",
        "    state_df = master_history[state].copy(deep=True)\n",
        "    state_df['state'] = state\n",
        "    state_df['region'] = region_names[i]\n",
        "    state_df.drop(state_df[state_df.date < '2020-03-05'].index, inplace=True)\n",
        "    state_df.drop(state_df[state_df.date > '2020-12-31'].index, inplace=True)\n",
        "    state_df = state_df.iloc[::-1]\n",
        "    region_df = pd.concat([region_df, state_df])\n",
        "  \n",
        "  region_df.reset_index(inplace=True)\n",
        "  history.append(region_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2MXnJd6-yyf"
      },
      "source": [
        "history[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQbJ9GVqyjhM"
      },
      "source": [
        "# find where to place ticks (start of each month) and set their labels\n",
        "month_ticks = ['Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "tick_indices = []\n",
        "\n",
        "curr_month = ''\n",
        "# pull out one state as an example for state_df before running this once\n",
        "for i, date in enumerate(state_df.date.tolist()):\n",
        "  month = date[5:7]\n",
        "  if month != curr_month:\n",
        "    tick_indices.append(i)\n",
        "    curr_month = month"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG1N77Yxw4Vu"
      },
      "source": [
        "###Case Count Trends: Positive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU76ni3Oyf0T"
      },
      "source": [
        "for div in history:\n",
        "  sns.set_style(\"whitegrid\")\n",
        "  sns.set_context(\"talk\", font_scale=3.1)\n",
        "  g = sns.relplot(\n",
        "      data=div, kind=\"line\", sort=False,\n",
        "      x='date', y='posCases', hue='state',\n",
        "      height=15, aspect=2.5, palette=\"Set2\"\n",
        "  ).set_axis_labels(\"Date (Month in 2020)\", \"Number of Cases\", labelpad=35)\n",
        "  g.set(xticks=tick_indices)\n",
        "  g.set_xticklabels(month_ticks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BovXnJtNBScH"
      },
      "source": [
        "###Case Count Trends: Negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ-nvbNf1HWV"
      },
      "source": [
        "for div in history:\n",
        "  sns.set_style(\"whitegrid\")\n",
        "  sns.set_context(\"talk\", font_scale=3.1)\n",
        "  g = sns.relplot(\n",
        "      data=div, kind=\"line\", sort=False,\n",
        "      x='date', y='negCases', hue='state',\n",
        "      height=15, aspect=2.5, palette=\"Set2\"\n",
        "  ).set_axis_labels(\"Date (Month in 2020)\", \"Number of Cases\", labelpad=35)\n",
        "  g.set(xticks=tick_indices)\n",
        "  g.set_xticklabels(month_ticks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpLVGFMM0-rB"
      },
      "source": [
        "###Data Quality Analysis\n",
        "Note: N/A=75 for all states after 10/27"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNccg99803ca"
      },
      "source": [
        "history_df = pd.concat([history[0], history[1], history[2], history[3], history[4], history[5], history[6], history[7], history[8]])\n",
        "history_df.sort_values(by=['state'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzanAXKi0eIg"
      },
      "source": [
        "# Title: Data Quality Distribution Across States\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.catplot(\n",
        "    data=history_df, kind=\"box\",\n",
        "    x=\"state\", y=\"dataQualityGrade\", \n",
        "    height=15, aspect=2.5, palette=\"Set2\"\n",
        ").set_axis_labels(\"State\", \"Data Quality Grade\", labelpad=35)\n",
        "g.set_xticklabels(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnBwnr2uFkEj"
      },
      "source": [
        "##Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGBj97ZDWjAJ"
      },
      "source": [
        "###Daily Average Sentiment Values for a given state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2rFTN6zP-dk"
      },
      "source": [
        "# Find out where to place x ticks and tick labels on graph\n",
        "month_ticks = ['Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "\n",
        "def xTicks(dates):\n",
        "  tick_indices = []\n",
        "  curr_month = ''\n",
        "  for i, date in enumerate(dates):\n",
        "    month = date[5:7]\n",
        "    if month != curr_month:\n",
        "      tick_indices.append(i)\n",
        "      curr_month = month\n",
        "  return tick_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcC5BwmPOixB"
      },
      "source": [
        "# given a state, graph its different sentiment values\n",
        "def graphStateSentiment(state):\n",
        "  date = daily_tweets[state]['date'].tolist()\n",
        "  tick_indices = xTicks(date)\n",
        "  length = len(date)\n",
        "  date = date + date + date + date\n",
        "  score = daily_tweets[state]['compoundSent'].tolist() + daily_tweets[state]['posSent'].tolist() + daily_tweets[state]['negSent'].tolist() + daily_tweets[state]['neutralSent'].tolist()\n",
        "  sentiment = ['Compound'] * length + ['Positive'] * length + ['Negative'] * length + ['Neutral'] * length\n",
        "\n",
        "  df = pd.DataFrame.from_dict({'date': date, 'Sentiment': sentiment, 'score': score})\n",
        "\n",
        "  # Title: \n",
        "  sns.set_style(\"whitegrid\")\n",
        "  sns.set_context(\"talk\", font_scale=3.1)\n",
        "  g = sns.relplot(\n",
        "      data=df, kind=\"line\", \n",
        "      x='date', y='score', hue='Sentiment',\n",
        "      height=15, aspect=2.5, palette=\"Set2\"\n",
        "  ).set_axis_labels(\"Date (Month in 2020)\", \"Score\", labelpad=35)\n",
        "  g.set(xticks=tick_indices)\n",
        "  g.set_xticklabels(month_ticks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J65vQWZYRI0N"
      },
      "source": [
        "graphStateSentiment('hi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u40dALNiSG0p"
      },
      "source": [
        "graphStateSentiment('wv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkxAHjwLU6tP"
      },
      "source": [
        "graphStateSentiment('or')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTyXgfokB9Bd"
      },
      "source": [
        "daily_tweets[state].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2xl_ySVqqw8"
      },
      "source": [
        "# find states with max average positive/negative sentiment values\n",
        "posmax = 0\n",
        "negmax = 0\n",
        "posmax_state = 'N/A'\n",
        "negmax_state = 'N/A'\n",
        "\n",
        "for state in states_lower:\n",
        "  means = daily_tweets[state].mean()\n",
        "  if means['compoundSent'] > posmax:\n",
        "    posmax = means['compoundSent']\n",
        "    posmax_state = state\n",
        "  if means['compoundSent'] < negmax:\n",
        "    negmax = means['compoundSent']\n",
        "    negmax_state = state\n",
        "    \n",
        "print(\"highest average positive sentiment: \", posmax_state, \" with mean \", posmax)\n",
        "print(\"highest average negative sentiment: \", negmax_state, \" with mean \", negmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7mj3exEWoeM"
      },
      "source": [
        "###Distribution of compound sentiment across all states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLF-_Wo-XZIZ"
      },
      "source": [
        "# Set up for box plot by combining data for all states' compound values\n",
        "compound_df = pd.DataFrame()\n",
        "for state in states_lower:\n",
        "  date = daily_tweets[state]['date'].tolist()\n",
        "  compound = daily_tweets[state]['compoundSent'].tolist()\n",
        "  state = [state] * len(date)\n",
        "  df = pd.DataFrame.from_dict({'date': date, 'state': state, 'compoundSent': compound})\n",
        "  compound_df = pd.concat([compound_df, df])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbEE0dvwXKiR"
      },
      "source": [
        "# Title: Distribution of Compound Sentiment Values by State\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.catplot(\n",
        "    data=compound_df, kind=\"box\",\n",
        "    x=\"state\", y=\"compoundSent\", \n",
        "    height=15, aspect=2.5, palette=\"Set2\"\n",
        ").set_axis_labels(\"State\", \"Sentiment Score\", labelpad=35)\n",
        "g.set_xticklabels(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtlEF4j5UZ0U"
      },
      "source": [
        "# **The Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLH2Gopjb8UT"
      },
      "source": [
        "##**[RUN OPTION 2]** Final I/O Setup\n",
        "Load I/O table from GDrive, split into train/validate/test sets, shuffle, split input/outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJTKHGULix-I"
      },
      "source": [
        "### By State (Option 1) - ignore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlSZNRAg6kA2"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# load input/output data from GDrive, save into io_dict: {state: df}\n",
        "# drop the date column\n",
        "# ------------------------------------------------------------------------------\n",
        "io_dict = {} # empty dict to fill with dfs\n",
        "for state in states_lower:\n",
        "  path = 'drive/My Drive/Thesis/5. IO Table/{}_IO.csv'.format(state)\n",
        "  io_dict[state] = pd.read_csv(path, index_col=0).drop(columns=['date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPu6YMfi8SYx"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# shuffle then split data set\n",
        "# 70:10:20 train:validate:test ratios (rounded)\n",
        "# ------------------------------------------------------------------------------\n",
        "train_dict = {}\n",
        "validate_dict = {}\n",
        "test_dict = {}\n",
        "\n",
        "for state in states_lower:\n",
        "  shuffle = io_dict[state].sample(frac=1) # shuffle\n",
        "  \n",
        "  tenth = io_dict[state].shape[0] / 10\n",
        "  train_dict[state] = io_dict[state].iloc[:int(tenth*7)]\n",
        "  validate_dict[state] = io_dict[state].iloc[int(tenth*7):int(tenth*8)]\n",
        "  test_dict[state] = io_dict[state].iloc[int(tenth*8):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igCbaGB9OWWu"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# separate the input and output into separate dfs\n",
        "# ------------------------------------------------------------------------------\n",
        "train_input = {}\n",
        "validate_input = {}\n",
        "test_input = {}\n",
        "\n",
        "train_output = {}\n",
        "validate_output = {}\n",
        "test_output = {}\n",
        "\n",
        "for state in states_lower:\n",
        "  train_input[state] = train_dict[state].iloc[:, :-3]\n",
        "  train_output[state] = train_dict[state].iloc[:, -3:]\n",
        "  validate_input[state] = validate_dict[state].iloc[:, :-3]\n",
        "  validate_output[state] = validate_dict[state].iloc[:, -3:]\n",
        "  test_input[state] = test_dict[state].iloc[:, :-3]\n",
        "  test_output[state] = test_dict[state].iloc[:, -3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvAsJw_wyUmE"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# convert to tensors\n",
        "# ------------------------------------------------------------------------------\n",
        "for state in states_lower:\n",
        "  train_input[state] = torch.FloatTensor(train_input[state].values)\n",
        "  train_output[state] = torch.FloatTensor(train_output[state].values)\n",
        "  validate_input[state] = torch.FloatTensor(validate_input[state].values)\n",
        "  validate_output[state] = torch.FloatTensor(validate_output[state].values)\n",
        "  test_input[state] = torch.FloatTensor(test_input[state].values)\n",
        "  test_output[state] = torch.FloatTensor(test_output[state].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tNVEP7ki1Ne"
      },
      "source": [
        "### Master DF (Option 2 -- go with this one)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8298N8C3i0wn"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# load master I/O df from GDrive, drop the date column\n",
        "# ------------------------------------------------------------------------------\n",
        "path = 'drive/My Drive/Thesis/5. IO Table/master_IO.csv'\n",
        "master_df = pd.read_csv(path, index_col=0).drop(columns=['date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vnoLqm27F0I"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# comparative option for evaluation: remove all tweet sentiment data\n",
        "# ------------------------------------------------------------------------------\n",
        "master_df = master_df.drop(['today_tweetCount', 'mean3_tweetCount', 'median3_tweetCount', 'mean7_tweetCount', 'median7_tweetCount'], axis=1)\n",
        "master_df = master_df.drop(['today_CompoundSent', 'today_posSent', 'today_negSent', 'today_neutralSent'], axis=1)\n",
        "master_df = master_df.drop(['mean3_CompoundSent', 'mean3_posSent', 'mean3_negSent', 'mean3_neutralSent'], axis=1)\n",
        "master_df = master_df.drop(['median3_CompoundSent', 'median3_posSent', 'median3_negSent', 'median3_neutralSent'], axis=1)\n",
        "master_df = master_df.drop(['mean7_CompoundSent', 'mean7_posSent', 'mean7_negSent', 'mean7_neutralSent'], axis=1)\n",
        "master_df = master_df.drop(['median7_CompoundSent', 'median7_posSent', 'median7_negSent', 'median7_neutralSent'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiaTbWIjkAcw"
      },
      "source": [
        "master_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_qO6rtofPph"
      },
      "source": [
        "test = master_df['predict3'].tolist()\n",
        "test.sort()\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X710OMwii_Tk"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# shuffle then split data set\n",
        "# 70:30 train:test ratio (rounded)\n",
        "# ------------------------------------------------------------------------------\n",
        "shuffle = master_df.sample(frac=1) # shuffle\n",
        "\n",
        "tenth = master_df.shape[0] / 10\n",
        "train = master_df.iloc[:int(tenth*7)]\n",
        "test = master_df.iloc[int(tenth*7):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjhoDQmAjOOz"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# separate the input and output into separate dfs\n",
        "# ------------------------------------------------------------------------------\n",
        "train_input = train.iloc[:, :-3]\n",
        "train_output = train.iloc[:, -3:]\n",
        "test_input = test.iloc[:, :-3]\n",
        "test_output = test.iloc[:, -3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPxx5ZXbjjCT"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# convert to tensors\n",
        "# ------------------------------------------------------------------------------\n",
        "train_input = torch.FloatTensor(train_input.values)\n",
        "train_output = torch.FloatTensor(train_output.values)\n",
        "test_input = torch.FloatTensor(test_input.values)\n",
        "test_output = torch.FloatTensor(test_output.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WggyjQWcaiW"
      },
      "source": [
        "##Define and train NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4crrOmD12Px9"
      },
      "source": [
        "Enable GPU under \"Edit\" > \"Notebook Settings\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEXU9r3v2wmy"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# move tensor to the GPU if available\n",
        "# ------------------------------------------------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_input = train_input.to(device)\n",
        "train_output = train_output.to(device)\n",
        "test_input = test_input.to(device)\n",
        "test_output = test_output.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S80Rh0J3gaQ"
      },
      "source": [
        "print(f\"Shape of tensor: {train_input.shape}\")\n",
        "print(f\"Datatype of tensor: {train_input.dtype}\")\n",
        "print(f\"Device tensor is stored on: {train_input.device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBoPYbQRcs3f"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# define model constructor\n",
        "# ------------------------------------------------------------------------------\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.hidden = torch.nn.Linear(input_size, hidden_size) # fc1 layer\n",
        "    self.predict = torch.nn.Linear(hidden_size, 3) # fc2 output layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    # introduce non-linearity to hidden layer's output (any neg# --> 0)\n",
        "    hidden = F.relu(self.hidden(x)) \n",
        "\n",
        "    # linear output\n",
        "    x = self.predict(hidden)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AD7KPaNPrnu"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# initialize model\n",
        "# ------------------------------------------------------------------------------\n",
        "input_size = train_input.size()[1] # number of features selected\n",
        "hidden_size = 21 # number of nodes/neurons in the hidden layer\n",
        "\n",
        "model = Net(input_size, hidden_size) # create the model\n",
        "criterion = torch.nn.MSELoss() # mean square error loss function\n",
        "model = model.to(device) # move to gpu if available \n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4) # adam optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM5egEGvUFxR"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# check test loss before model training\n",
        "# ------------------------------------------------------------------------------\n",
        "model.eval() # switch to eval mode (so doesn't learn new weights)\n",
        "y_pred = model(test_input)\n",
        "before_train = criterion(y_pred.squeeze(), test_output.squeeze())\n",
        "print('Test loss before training' , before_train.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xd8GmB_kBqo"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# split up y_pred and test_output tensors for more individual analysis pre-train\n",
        "# ------------------------------------------------------------------------------\n",
        "y_pred3 = y_pred[:, 0]\n",
        "y_pred7 = y_pred[:, 1]\n",
        "y_pred14 = y_pred[:, 2]\n",
        "\n",
        "test_output3 = test_output[:, 0]\n",
        "test_output7 = test_output[:, 1]\n",
        "test_output14 = test_output[:, 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_szSDFGkF80"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# find mean squared error loss values by day\n",
        "# ------------------------------------------------------------------------------\n",
        "before_accuracy3 = criterion(y_pred3, test_output3)\n",
        "before_accuracy7 = criterion(y_pred7, test_output7)\n",
        "before_accuracy14 = criterion(y_pred14, test_output14)\n",
        "\n",
        "print('Pre-training MSELoss for 3 day prediction:' , before_accuracy3.item())\n",
        "print('Pre-training MSELoss for 7 day prediction:' , before_accuracy7.item())\n",
        "print('Pre-training MSELoss for 14 day prediction:' , before_accuracy14.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lsv9c3xUPcN"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# train model\n",
        "# ------------------------------------------------------------------------------\n",
        "model.train() # switch back to training mode\n",
        "epochs = 10000 # what should i set this to?\n",
        "errors = [] # maintain record to keep loss for each epoch\n",
        "errors3 = [] # errors for 3 day prediction\n",
        "errors7 = [] # errors for 7 day prediction\n",
        "errors14 = [] # # errors for 14 day prediction\n",
        "\n",
        "# split up train outputs for comparison during training\n",
        "train_output3 = train_output[:, 0]\n",
        "train_output7 = train_output[:, 1]\n",
        "train_output14 = train_output[:, 2]\n",
        "\n",
        "index = 0\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad() # clear gradients before backpropagation\n",
        "  # Forward pass\n",
        "  y_pred = model(train_input)\n",
        "  y_pred3 = y_pred[:, 0]\n",
        "  y_pred7 = y_pred[:, 1]\n",
        "  y_pred14 = y_pred[:, 2]\n",
        "  # Compute Loss\n",
        "  loss = criterion(y_pred.squeeze(), train_output.squeeze())\n",
        "  errors.append(loss.item())\n",
        "  errors3.append(criterion(y_pred3, train_output3).item())\n",
        "  errors7.append(criterion(y_pred7, train_output7).item())\n",
        "  errors14.append(criterion(y_pred14, train_output14).item())\n",
        "  if index%10 == 0:\n",
        "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "  index+=1\n",
        "  # Compute other measures of accuracy?\n",
        "  # Backward pass\n",
        "  loss.backward() # backpropagation, compute gradients\n",
        "  optimizer.step() # apply gradients to update weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEhdlaoRZU3d"
      },
      "source": [
        "## Evaluate Accuracy: Absolute Loss (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRZ5NyKhUxpu"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# evaluate post-training: overall accuracy\n",
        "# ------------------------------------------------------------------------------\n",
        "model.eval()\n",
        "y_pred = model(test_input)\n",
        "after_train = criterion(y_pred.squeeze(), test_output.squeeze())\n",
        "print('Test loss after training:' , after_train.item())\n",
        "print('Compare to loss before training: ', before_train.item())\n",
        "print('This is a difference of: ', before_train.item()/after_train.item(), 'times')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXmpS0Vn4-yc"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# split up y_pred and test_output tensors for more individual analysis\n",
        "# ------------------------------------------------------------------------------\n",
        "y_pred3 = y_pred[:, 0]\n",
        "y_pred7 = y_pred[:, 1]\n",
        "y_pred14 = y_pred[:, 2]\n",
        "\n",
        "test_output3 = test_output[:, 0]\n",
        "test_output7 = test_output[:, 1]\n",
        "test_output14 = test_output[:, 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIvt8KeaNiKy"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# find mean squared error loss values by day\n",
        "# ------------------------------------------------------------------------------\n",
        "accuracy3 = criterion(y_pred3, test_output3)\n",
        "accuracy7 = criterion(y_pred7, test_output7)\n",
        "accuracy14 = criterion(y_pred14, test_output14)\n",
        "\n",
        "print('MSELoss for 3 day prediction:' , accuracy3.item())\n",
        "print('MSELoss for 7 day prediction:' , accuracy7.item())\n",
        "print('MSELoss for 14 day prediction:' , accuracy14.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAMuqc4BiIcf"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# find improvement for MSE loss values per day\n",
        "# ------------------------------------------------------------------------------\n",
        "print('3 day: improvement of: ', (before_accuracy3/accuracy3).item(), 'times')\n",
        "print('7 day: improvement of: ', (before_accuracy7/accuracy7).item(), 'times')\n",
        "print('14 day: improvement of: ', (before_accuracy14/accuracy14).item(), 'times')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBE2ku0WkNLO"
      },
      "source": [
        "##Evaluation: Relative Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEq5EN8RNULX"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# sort test output values in increasing order for more intuitive graphing purposes\n",
        "# ------------------------------------------------------------------------------\n",
        "test_output3_sort, idx3 = torch.sort(test_output3)\n",
        "test_output7_sort, idx7 = torch.sort(test_output7)\n",
        "test_output14_sort, idx14 = torch.sort(test_output14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt5xj7twO4zJ"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# function that takes in a list and sorts it according to a second list that represents index order\n",
        "# ------------------------------------------------------------------------------ \n",
        "def sortByIndex(arr, idx):\n",
        "  sorted_list = []\n",
        "  for i in idx:\n",
        "    sorted_list.append(arr[i].item())\n",
        "  return np.asarray(sorted_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW4-koiNPvLu"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# sort predicted values according to new order of test output values\n",
        "# ------------------------------------------------------------------------------\n",
        "y_pred3_sort = sortByIndex(y_pred3, idx3)\n",
        "y_pred7_sort = sortByIndex(y_pred7, idx7)\n",
        "y_pred14_sort = sortByIndex(y_pred14, idx14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yyF5r3HRQIM"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# see where super early numbers are (0-2)\n",
        "# ------------------------------------------------------------------------------\n",
        "(test_output3_sort == 2).nonzero(as_tuple=True)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngFNKej3myIq"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# first remove values where output=0 in relative3 (just 4 examples of super early on)\n",
        "# ------------------------------------------------------------------------------\n",
        "test_output3_cut = test_output3_sort[5:].cpu().numpy()\n",
        "test_output7_cut = test_output7_sort[5:].cpu().numpy()\n",
        "test_output14_cut = test_output14_sort[5:].cpu().numpy()\n",
        "y_pred3_cut = y_pred3_sort[5:]\n",
        "y_pred7_cut = y_pred7_sort[5:]\n",
        "y_pred14_cut = y_pred14_sort[5:]\n",
        "\n",
        "relative3 = np.abs(test_output3_cut - y_pred3_cut)/test_output3_cut * 100\n",
        "relative7 = np.abs(test_output7_cut - y_pred7_cut)/test_output7_cut * 100\n",
        "relative14 = np.abs(test_output14_cut - y_pred14_cut)/test_output14_cut * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jHkT3UediHg"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# check largest error value for a general sense / plausability \n",
        "# ------------------------------------------------------------------------------\n",
        "idx = np.argmax(relative3)\n",
        "print(idx, test_output3_cut[idx], y_pred3_cut[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TVqQczjpfps"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# find mean loss values by day\n",
        "# ------------------------------------------------------------------------------\n",
        "relative_accuracy3 = np.mean(relative3)\n",
        "relative_accuracy7 = np.mean(relative7)\n",
        "relative_accuracy14 = np.mean(relative14)\n",
        "\n",
        "print('Mean relative error for 3 day prediction:' , relative_accuracy3.item(), '%')\n",
        "print('Mean relative error for 7 day prediction:' , relative_accuracy7.item(), '%')\n",
        "print('Mean relative error for 14 day prediction:' , relative_accuracy14.item(), '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thyeqxLK36t6"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# find mean loss values by day\n",
        "# ------------------------------------------------------------------------------\n",
        "relative_accuracy3 = np.mean(relative3[100:])\n",
        "relative_accuracy7 = np.mean(relative7[100:])\n",
        "relative_accuracy14 = np.mean(relative14[100:])\n",
        "\n",
        "print('Mean relative error for 3 day prediction:' , relative_accuracy3.item(), '%')\n",
        "print('Mean relative error for 7 day prediction:' , relative_accuracy7.item(), '%')\n",
        "print('Mean relative error for 14 day prediction:' , relative_accuracy14.item(), '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABMfNRPu3icx"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# find mean loss values by day\n",
        "# ------------------------------------------------------------------------------\n",
        "relative_accuracy3 = np.mean(relative3[415:])\n",
        "relative_accuracy7 = np.mean(relative7[415:])\n",
        "relative_accuracy14 = np.mean(relative14[415:])\n",
        "\n",
        "print('Mean relative error for 3 day prediction:' , relative_accuracy3.item(), '%')\n",
        "print('Mean relative error for 7 day prediction:' , relative_accuracy7.item(), '%')\n",
        "print('Mean relative error for 14 day prediction:' , relative_accuracy14.item(), '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIJ50sJ_5SSt"
      },
      "source": [
        "Some notes:\n",
        "After about the earliest 100-150ish predictions, accuracy increases dramatically. Corresponds with when the actual case counts pass the early couple hundreds. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjso2pdAZ92Z"
      },
      "source": [
        "##Evaluation: Find % Correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnT0gTxN7WsH"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# define function which takes in array of relative accuracies and returns how \n",
        "# many predictions fall within 1%, 5%, 25%, and 50%. \n",
        "# ------------------------------------------------------------------------------\n",
        "def countThresholds(arr, idx):\n",
        "  counts = {1: 0, 5: 0, 25: 0, 50: 0, 100: 0, 'else': 0}\n",
        "  for x in arr:\n",
        "    if x <= 1:\n",
        "      counts[1] += 1\n",
        "    elif x <= 5:\n",
        "      counts[5] += 1\n",
        "    elif x <= 25:\n",
        "      counts[25] += 1\n",
        "    elif x <= 50:\n",
        "      counts[50] += 1\n",
        "    elif x <= 100:\n",
        "      counts[100] += 1\n",
        "    else:\n",
        "      counts['else'] += 1\n",
        "\n",
        "  total_count = len(arr)\n",
        "  counts['within 1%'] = counts[1] / total_count\n",
        "  counts['within 5%'] = (counts[1] + counts[5]) / total_count\n",
        "  counts['within 25%'] = (counts[1] + counts[5] + counts[25]) / total_count\n",
        "  counts['within 50%'] = (counts[1] + counts[5] + counts[25] + counts[50]) / total_count\n",
        "  counts['within 100%'] = (counts[1] + counts[5] + counts[25] + counts[50] + counts[100]) / total_count\n",
        "  return pd.DataFrame(counts, index=[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgeIlOtH8d40"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# find thresholds for predictions \n",
        "# ------------------------------------------------------------------------------\n",
        "thresholds3 = countThresholds(relative3, '3-day')\n",
        "thresholds7 = countThresholds(relative7, '7-day')\n",
        "thresholds14 = countThresholds(relative14, '14-day')\n",
        "\n",
        "thresholds_df = pd.concat([thresholds3, thresholds7, thresholds14])\n",
        "thresholds_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n8-TqXHZ5ty"
      },
      "source": [
        "##Evaluation: Plot errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ2Ia_AC6MMf"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# plot total MSE\n",
        "# ------------------------------------------------------------------------------\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=np.log10(errors), kind=\"line\",\n",
        "    height=15, aspect=2.5, palette=\"Set2\"\n",
        ").set_axis_labels(\"Epochs\", \"Log Loss\", labelpad=35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qoL7x2277T-"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# prepare df for multi-line plots\n",
        "# ------------------------------------------------------------------------------\n",
        "epochs = [*range(10000)] * 3\n",
        "errors_daily = np.concatenate([np.log10(errors3), np.log10(errors7), np.log10(errors14)])\n",
        "prediction = ['3 day'] * 10000 + ['7 day'] * 10000 + ['14 day'] * 10000\n",
        "errors_df = pd.DataFrame.from_dict({'epochs': epochs, 'errors': errors_daily, 'Prediction': prediction})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k98Gy6p6W5P"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# plot 3, 7, and 14 day MSE values against each other\n",
        "# ------------------------------------------------------------------------------\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=errors_df, kind=\"line\", \n",
        "    x='epochs', y='errors', hue='Prediction',\n",
        "    height=15, aspect=2.3, palette=\"Set2\"\n",
        ").set_axis_labels(\"Epoch\", \"Log Loss\", labelpad=35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4J8xtGO8sTr"
      },
      "source": [
        "##Evaluation: Plot Relative Errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31NX1Wq46a0x"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# plot relative loss values against each other\n",
        "# ------------------------------------------------------------------------------\n",
        "index = [*range(4195)] * 3\n",
        "relative_errors = np.concatenate([relative3/100, relative7/100, relative14/100])\n",
        "prediction = ['3 day'] * 4195 + ['7 day'] * 4195 + ['14 day'] * 4195\n",
        "errors_df = pd.DataFrame.from_dict({'index': index, 'errors': relative_errors, 'Prediction': prediction})\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=errors_df, kind=\"line\", \n",
        "    x='index', y='errors', hue='Prediction',\n",
        "    height=15, aspect=2.3, palette=\"Set2\"\n",
        ").set_axis_labels(\"\", \"Relative Error\", labelpad=35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecnPuER18vl9"
      },
      "source": [
        "##Evaluation: Plot Predicted on top of Actual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j-ilNg0otPF"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# graph 3 day predicted and actual\n",
        "# ------------------------------------------------------------------------------\n",
        "dataCount = len(test_output3_cut.tolist())\n",
        "data = test_output3_cut.tolist() + y_pred3_cut.tolist()\n",
        "category = ['actual'] * dataCount + ['predicted'] * dataCount\n",
        "index = list(range(dataCount)) * 2\n",
        "\n",
        "predict3_df = pd.DataFrame.from_dict({'index': index, 'data': data, 'category': category})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QavMTKgbwWVc"
      },
      "source": [
        "current_palette = sns.color_palette(\"Set2\")\n",
        "first = current_palette[5]\n",
        "second = current_palette[2]\n",
        "sns.set_palette([first, second])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XjWgoR-TRn1"
      },
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=predict3_df,\n",
        "    x='index', y='data', hue='category', style='category',\n",
        "    height=15, aspect=2.3, linewidth=0, markers=['o', '*'], s=1000\n",
        ").set_axis_labels(\"\", \"Number of Cases\", labelpad=35)\n",
        "\n",
        "for lh in g._legend.legendHandles: \n",
        "    lh.set_alpha(1)\n",
        "    lh._sizes = [600] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxDHGczq0w6C"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# graph 7 day predicted and actual\n",
        "# ------------------------------------------------------------------------------\n",
        "dataCount = len(test_output7_cut.tolist())\n",
        "data = test_output7_cut.tolist() + y_pred7_cut.tolist()\n",
        "category = ['actual'] * dataCount + ['predicted'] * dataCount\n",
        "index = list(range(dataCount)) * 2\n",
        "\n",
        "predict7_df = pd.DataFrame.from_dict({'index': index, 'data': data, 'category': category})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZQqnKNl01_2"
      },
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=predict7_df,\n",
        "    x='index', y='data', hue='category', style='category',\n",
        "    height=15, aspect=2.3, linewidth=0, markers=['o', '*'], s=1000\n",
        ").set_axis_labels(\"\", \"Number of Cases\", labelpad=35)\n",
        "\n",
        "for lh in g._legend.legendHandles: \n",
        "    lh.set_alpha(1)\n",
        "    lh._sizes = [600] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6xKb1oJ0_eT"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# graph 14 day predicted and actual\n",
        "# ------------------------------------------------------------------------------\n",
        "dataCount = len(test_output14_cut.tolist())\n",
        "data = test_output14_cut.tolist() + y_pred14_cut.tolist()\n",
        "category = ['actual'] * dataCount + ['predicted'] * dataCount\n",
        "index = list(range(dataCount)) * 2\n",
        "\n",
        "predict14_df = pd.DataFrame.from_dict({'index': index, 'data': data, 'category': category})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4rSVS1w1Ip3"
      },
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    data=predict14_df,\n",
        "    x='index', y='data', hue='category', style='category',\n",
        "    height=15, aspect=2.3, linewidth=0, markers=['o', '*'], s=1000\n",
        ").set_axis_labels(\"\", \"Number of Cases\", labelpad=35)\n",
        "\n",
        "for lh in g._legend.legendHandles: \n",
        "    lh.set_alpha(1)\n",
        "    lh._sizes = [600] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L7LvlxM80sy"
      },
      "source": [
        "##Evaluation: Plot Predicted v. Actual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gExfTi691gk2"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# reset color palette\n",
        "# ------------------------------------------------------------------------------\n",
        "# current_palette = sns.color_palette(\"Set2\")\n",
        "sns.set_palette(\"Set2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6i6GASyq3SO"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# graph 3 day predicted v. actual\n",
        "# ------------------------------------------------------------------------------\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    x=test_output3_cut.tolist(), y=y_pred3_cut.tolist(),\n",
        "    height=15, aspect=2.3\n",
        ").set_axis_labels(\"Actual\", \"Predicted\", labelpad=35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E87hxo9wS_OP"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# graph 7 day predicted v. actual\n",
        "# ------------------------------------------------------------------------------\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    x=test_output7_cut.tolist(), y=y_pred7_cut.tolist(),\n",
        "    height=15, aspect=2.3\n",
        ").set_axis_labels(\"Actual\", \"Predicted\", labelpad=35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iJe4HapTBhr"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# graph 14 day predicted v. actual\n",
        "# ------------------------------------------------------------------------------\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\", font_scale=3.1)\n",
        "g = sns.relplot(\n",
        "    x=test_output14_cut.tolist(), y=y_pred14_cut.tolist(),\n",
        "    height=15, aspect=2.3\n",
        ").set_axis_labels(\"Actual\", \"Predicted\", labelpad=35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN-59oLQ84hz"
      },
      "source": [
        "##Old Evaluation Plots (no longer used)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVzeppYmU5Qm"
      },
      "source": [
        "# evaluate with some plots (code from Batista medium article)\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "def plotcharts(errors):\n",
        "  errors = np.array(errors)\n",
        "\n",
        "  plt.figure(figsize=(12, 5))\n",
        "\n",
        "  error_graph = plt.subplot(1, 2, 1) # nrows, ncols, index\n",
        "  error_graph.set_title('Errors')\n",
        "  plt.plot(errors, '-')\n",
        "  plt.xlabel('Epochs')\n",
        "\n",
        "  predict3_graph = plt.subplot(1, 2, 2)\n",
        "  predict3_graph.set_title('3 Day Prediction')\n",
        "  a = plt.plot(test_output3.detach().cpu().numpy(), 'yo', label='Real')\n",
        "  plt.setp(a, markersize=10)\n",
        "  a = plt.plot(y_pred3.detach().cpu().numpy(), 'b+', label='Predicted')\n",
        "  plt.setp(a, markersize=10)\n",
        "  plt.legend(loc=7)\n",
        "\n",
        "plotcharts(errors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elZlQf5B6T1F"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "predict7_graph = plt.subplot(1, 2, 1)\n",
        "predict7_graph.set_title('7 Day Prediction')\n",
        "a = plt.plot(test_output7.detach().cpu().numpy(), 'yo', label='Real')\n",
        "plt.setp(a, markersize=10)\n",
        "a = plt.plot(y_pred7.detach().cpu().numpy(), 'b+', label='Predicted')\n",
        "plt.setp(a, markersize=10)\n",
        "plt.legend(loc=7)\n",
        "\n",
        "predict14_graph = plt.subplot(1, 2, 2)\n",
        "predict14_graph.set_title('14 Day Prediction')\n",
        "a = plt.plot(test_output14.detach().cpu().numpy(), 'yo', label='Real')\n",
        "plt.setp(a, markersize=10)\n",
        "a = plt.plot(y_pred14.detach().cpu().numpy(), 'b+', label='Predicted')\n",
        "plt.setp(a, markersize=10)\n",
        "plt.legend(loc=7)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdhm0q6S6HCA"
      },
      "source": [
        "##Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHXgu6DmVcDm"
      },
      "source": [
        "Tutorial used for this entire section: https://medium.com/@andreluiz_4916/pytorch-neural-networks-to-predict-matches-results-in-soccer-championships-part-ii-3d02b2ddd538\n",
        "\n",
        "Params to change to try to improve predictions:\n",
        "1. Learning rate: float values between 0 and 1.\n",
        "2. Momentum rate: float values between 0 and 1.\n",
        "3. Number of hidden layers: change the structure of your model adding more hidden layers.\n",
        "4. Number of neurons/nodes in the hidden layer: integer values between 1 and your imagination.\n",
        "5. Epochs: integer values between 1 and your level of patience to wait the training process to be finished.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_bqW9H-kCCa"
      },
      "source": [
        "# **Miscellaneous**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2giFZuGmeXkP"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaDDIJZzgHYN"
      },
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# FIX NAMES OF FILES (made copies with princeton account bc storage space issues in the personal account)\n",
        "# ------------------------------------------------------------------------------\n",
        "location_files = glob.glob(\"drive/My Drive/Thesis/Scrap/Tweets By Location (first couple dates, old method with geocode)/*.csv\")\n",
        "for file in sorted(location_files):\n",
        "  state = file[-13:-11]\n",
        "  new_name = \"drive/My Drive/Thesis/Scrap/Tweets By Location (first couple dates, old method with geocode)/{}_tweets.csv\".format(state)\n",
        "  os.rename(file, new_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}